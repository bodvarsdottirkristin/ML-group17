{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32626768",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.linear_model as lm\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.gridspec as gridspec\n",
    "from sklearn.model_selection import train_test_split, KFold, LeaveOneOut\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Plotting style\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_theme(font_scale=1.)\n",
    "\n",
    "# Define parameters\n",
    "seed = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ebd35896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge\n",
    "\n",
    "# Plotting style\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_theme(font_scale=1.)\n",
    "\n",
    "def setup_storage_for_experiment(K_outer, K_inner, num_hyperparams):\n",
    "    # Setup storage for the optimal hyperparameters found from the inner CV\n",
    "    optimal_hyperparameters = np.empty(K_outer)\n",
    "\n",
    "    # Setup storage for model coefficients and errors for each experiment in all inner folds\n",
    "    ws_inner = np.empty((M + 1, K_outer, K_inner, num_hyperparams))\n",
    "    train_errors_inner = np.empty((K_outer, K_inner, num_hyperparams))\n",
    "    test_errors_inner = np.empty((K_outer, K_inner, num_hyperparams))\n",
    "\n",
    "    # Setup storage for model coefficients for each experiment in all outer folds\n",
    "    ws_outer = {\n",
    "        'not regularized': np.empty((M + 1, K_outer)),\n",
    "        'regularized': np.empty((M + 1, K_outer))\n",
    "    }\n",
    "    # Setup storage for errors as a dictionary\n",
    "    errors_outer = {\n",
    "        'train': {\n",
    "            'baseline (no features)': np.empty((K_outer, 1)), \n",
    "            'not regularized': np.empty((K_outer, 1)),\n",
    "            'regularized': np.empty((K_outer, 1))\n",
    "        },\n",
    "        'test': {\n",
    "            'baseline (no features)': np.empty((K_outer, 1)), \n",
    "            'not regularized': np.empty((K_outer, 1)),\n",
    "            'regularized': np.empty((K_outer, 1))\n",
    "        }\n",
    "    }\n",
    "    return optimal_hyperparameters, ws_inner, train_errors_inner, test_errors_inner, ws_outer, errors_outer\n",
    "\n",
    "def print_regularization_results(errors):\n",
    "    # Display results\n",
    "    print(\"Linear regression without regularization:\")\n",
    "    print(f\"- Training error: \\t{errors['train']['not regularized'].mean():.4f}\")\n",
    "    print(f\"- Test error: \\t\\t{errors['test']['not regularized'].mean():.4f}\")\n",
    "    print(f\"- R^2 train: \\t\\t{(errors['train']['baseline (no features)'].sum() - errors['train']['not regularized'].sum()) / errors['train']['baseline (no features)'].sum():.4f}\")\n",
    "    print(f\"- R^2 test: \\t\\t{(errors['test']['baseline (no features)'].sum() - errors['test']['not regularized'].sum()) / errors['test']['baseline (no features)'].sum():.4f}\\n\")\n",
    "\n",
    "    print(\"Regularized linear regression:\")\n",
    "    print(f\"- Training error: \\t{errors['train']['regularized'].mean():.4f}\")\n",
    "    print(f\"- Test error: \\t\\t{errors['test']['regularized'].mean():.4f}\")\n",
    "    print(f\"- R^2 train: \\t\\t{(errors['train']['baseline (no features)'].sum() - errors['train']['regularized'].sum()) / errors['train']['baseline (no features)'].sum():.4f}\")\n",
    "    print(f\"- R^2 test: \\t\\t{(errors['test']['baseline (no features)'].sum() - errors['test']['regularized'].sum()) / errors['test']['baseline (no features)'].sum():.4f}\\n\")\n",
    "\n",
    "def get_grid_points(x_min, x_max, y_min, y_max, delta=5e-3):\n",
    "    # Create a grid of points with the specified resolution\n",
    "    xx = np.arange(x_min, x_max, delta)\n",
    "    yy = np.arange(y_min, y_max, delta)\n",
    "    # Make a mesh-grid that spans the grid-range defined\n",
    "    grid = np.stack(np.meshgrid(xx, yy))\n",
    "    return grid, xx, yy\n",
    "\n",
    "def plot_decision_boundary(predict_function, X, threshold=None, ax=None, fig=None, cmap='RdBu_r'):\n",
    "    # Set grid range based on the data\n",
    "    grid_range = [X[:, 0].min(), X[:, 0].max(), X[:, 1].min(), X[:, 1].max()]  # [x_min, x_max, y_min, y_max]\n",
    "    # Add 10% margin to the grid range to ensure points on the edge are included\n",
    "    margin_x = 0.1 * (grid_range[1] - grid_range[0])\n",
    "    margin_y = 0.1 * (grid_range[3] - grid_range[2])\n",
    "    grid_range[0] -= margin_x\n",
    "    grid_range[1] += margin_x\n",
    "    grid_range[2] -= margin_y\n",
    "    grid_range[3] += margin_y\n",
    "\n",
    "    # Get grid points\n",
    "    grid, xx, yy = get_grid_points(*grid_range, delta=5e-3)\n",
    "    # Reshape grid to a list of points\n",
    "    grid_points = np.reshape(grid, (2, -1)).T\n",
    "\n",
    "    # Compute model predictions on the grid points (i.e. the probability of class 1)\n",
    "    grid_predictions = predict_function(grid_points)\n",
    "\n",
    "    # Reshape the predictions back to the grid shape\n",
    "    decision_boundary = np.reshape(grid_predictions, (len(yy), len(xx)))\n",
    "\n",
    "    # Plot the decision boundary\n",
    "    img = ax.imshow(decision_boundary, extent=grid_range, origin='lower', cmap=cmap, alpha=0.5, vmin=0, vmax=1)\n",
    "    fig.colorbar(img, ax=ax)\n",
    "    if threshold is not None:\n",
    "        ax.contour(grid[0], grid[1], decision_boundary, levels=[threshold], colors='k')\n",
    "    ax.grid(False)\n",
    "    ax.set_aspect('auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a268dbc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row.names</th>\n",
       "      <th>sbp</th>\n",
       "      <th>tobacco</th>\n",
       "      <th>ldl</th>\n",
       "      <th>adiposity</th>\n",
       "      <th>famhist</th>\n",
       "      <th>typea</th>\n",
       "      <th>obesity</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>age</th>\n",
       "      <th>chd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>160</td>\n",
       "      <td>12.00</td>\n",
       "      <td>5.73</td>\n",
       "      <td>23.11</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>25.30</td>\n",
       "      <td>97.20</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>144</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.41</td>\n",
       "      <td>28.61</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>28.87</td>\n",
       "      <td>2.06</td>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>118</td>\n",
       "      <td>0.08</td>\n",
       "      <td>3.48</td>\n",
       "      <td>32.28</td>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>29.14</td>\n",
       "      <td>3.81</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>170</td>\n",
       "      <td>7.50</td>\n",
       "      <td>6.41</td>\n",
       "      <td>38.03</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>31.99</td>\n",
       "      <td>24.26</td>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>134</td>\n",
       "      <td>13.60</td>\n",
       "      <td>3.50</td>\n",
       "      <td>27.78</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>25.99</td>\n",
       "      <td>57.34</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row.names  sbp  tobacco   ldl  adiposity  famhist  typea  obesity  alcohol  \\\n",
       "0          1  160    12.00  5.73      23.11        1     49    25.30    97.20   \n",
       "1          2  144     0.01  4.41      28.61        0     55    28.87     2.06   \n",
       "2          3  118     0.08  3.48      32.28        1     52    29.14     3.81   \n",
       "3          4  170     7.50  6.41      38.03        1     51    31.99    24.26   \n",
       "4          5  134    13.60  3.50      27.78        1     60    25.99    57.34   \n",
       "\n",
       "   age chd  \n",
       "0   52   1  \n",
       "1   63   1  \n",
       "2   46   0  \n",
       "3   58   1  \n",
       "4   49   1  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initial data clean\n",
    "df = pd.read_csv('data/SAHeart.csv')\n",
    "\n",
    "# Encode Categorical values\n",
    "df['chd'] = pd.Categorical(df['chd'])\n",
    "df['famhist'] = df['famhist'].map({'Absent':0, 'Present':1}) #pd.Categorical(df['famhist'].map({'Absent':0, 'Present':1})) # One-of-K coding\n",
    "# Filter outliers if we need to??\n",
    "#df = df[~( (df[''] > 2) | (df[''] > 1) )]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7595d150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What to predict for Regression\n",
    "X = df.drop(columns=['ldl', 'chd']).values\n",
    "y = df['ldl'].values\n",
    "\n",
    "N, M = X.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd1da3f",
   "metadata": {},
   "source": [
    "## 2 - Regularize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f3193bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression without regularization:\n",
      "- Training error: \t3.3238\n",
      "- Test error: \t\t3.5264\n",
      "- R^2 train: \t\t0.2226\n",
      "- R^2 test: \t\t0.1644\n",
      "\n",
      "Regularized linear regression:\n",
      "- Training error: \t3.3572\n",
      "- Test error: \t\t3.5088\n",
      "- R^2 train: \t\t0.2148\n",
      "- R^2 test: \t\t0.1686\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Create crossvalidation partition for model evaluation\n",
    "K_outer = 5\n",
    "CV_outer = KFold(K_outer, shuffle=True)\n",
    "\n",
    "# Create crossvalidation partition for hyperparameter tuning\n",
    "K_inner = 10\n",
    "CV_inner = KFold(K_inner, shuffle=True)\n",
    "\n",
    "# Values of regularization parameter lambda to test in the inner loop\n",
    "lambdas = np.logspace(-5, 8, 14)\n",
    "\n",
    "# Setup storage for the experiment\n",
    "optimal_regularization_strengths, ws_inner, train_errors_inner, test_errors_inner, ws_outer, errors_outer = setup_storage_for_experiment(K_outer, K_inner, len(lambdas))\n",
    "\n",
    "# Run two-layer cross-validation\n",
    "for outer_fold_idx, (outer_train_index, outer_test_index) in enumerate(CV_outer.split(X, y)):\n",
    "    # Extract training and test set for the current outer CV fold\n",
    "    X_train_outer, y_train_outer = X[outer_train_index], y[outer_train_index]\n",
    "    X_test_outer, y_test_outer = X[outer_test_index], y[outer_test_index]\n",
    "\n",
    "    # Loop over inner cross-validation folds\n",
    "    for inner_fold_idx, (inner_train_index, inner_test_index) in enumerate(CV_inner.split(X_train_outer, y_train_outer)):\n",
    "\n",
    "        # Extract training and validation set for current inner CV fold\n",
    "        X_train_inner, y_train_inner = X_train_outer[inner_train_index], y_train_outer[inner_train_index]\n",
    "        X_test_inner, y_test_inner = X_train_outer[inner_test_index], y_train_outer[inner_test_index]\n",
    "\n",
    "        # Compute the mean and standard deviation of the inner training data split, then standardize training and test sets\n",
    "        ### BEGIN SOLUTION\n",
    "        # Compute the mean and standard deviation of the inner training data split\n",
    "        mu_inner = np.mean(X_train_inner, axis=0)\n",
    "        sigma_inner = np.std(X_train_inner, axis=0)\n",
    "        \n",
    "        # Standardize the inner training set and validation set\n",
    "        X_train_inner = (X_train_inner - mu_inner) / sigma_inner\n",
    "        X_test_inner = (X_test_inner - mu_inner) / sigma_inner\n",
    "        ### END SOLUTION\n",
    "\n",
    "        # Loop over all values of lambda\n",
    "        for lambda_idx, regularization_strength in enumerate(lambdas):\n",
    "\n",
    "            # Create and fit the model\n",
    "            ### BEGIN SOLUTION\n",
    "            model = Ridge(alpha=regularization_strength)\n",
    "            model.fit(X_train_inner, y_train_inner)\n",
    "            ### END SOLUTION\n",
    "\n",
    "            # Store the model coefficients for each value of lambda in the inner folds\n",
    "            ws_inner[:, outer_fold_idx, inner_fold_idx, lambda_idx] = [model.intercept_] + model.coef_.flatten().tolist()\n",
    "\n",
    "            # Compute and store the training and validation error\n",
    "            train_errors_inner[outer_fold_idx, inner_fold_idx, lambda_idx] = np.mean((y_train_inner - model.predict(X_train_inner))**2, axis=0)\n",
    "            test_errors_inner[outer_fold_idx, inner_fold_idx, lambda_idx] = np.mean((y_test_inner - model.predict(X_test_inner))**2, axis=0)\n",
    "    \n",
    "    # Determine the optimal value of lambda that gives the lowest test error on average from the inner folds\n",
    "    ### BEGIN SOLUTION\n",
    "    optimal_hyperparameter_idx = np.argmin(np.mean(test_errors_inner[outer_fold_idx], axis=0))\n",
    "    optimal_hyperparameter = lambdas[optimal_hyperparameter_idx]\n",
    "    ### END SOLUTION\n",
    "\n",
    "    # Store the optimal regularization strength for the current outer fold\n",
    "    optimal_regularization_strengths[outer_fold_idx] = optimal_hyperparameter\n",
    "\n",
    "    # Compute the mean and standard deviation of the outer training data split, then standardize the training and test sets\n",
    "    ### BEGIN SOLUTION\n",
    "    # Compute the mean and standard deviation of the outer training data split\n",
    "    mu_outer = np.mean(X_train_outer, axis=0)\n",
    "    sigma_outer = np.std(X_train_outer, axis=0)\n",
    "\n",
    "    # Standardize the outer training set and test set\n",
    "    X_train_outer = (X_train_outer - mu_outer) / sigma_outer\n",
    "    X_test_outer = (X_test_outer - mu_outer) / sigma_outer\n",
    "    ### END SOLUTION\n",
    "\n",
    "    # Create and fit the model with the optimal lambda on the entire outer training set\n",
    "    ### BEGIN SOLUTION\n",
    "    model = Ridge(alpha=optimal_hyperparameter)\n",
    "    model.fit(X_train_outer, y_train_outer)\n",
    "    ### END SOLUTION\n",
    "\n",
    "    # Store the model coefficients for the regularized model\n",
    "    ws_outer['regularized'][:, outer_fold_idx] = [model.intercept_] + model.coef_.flatten().tolist()\n",
    "    # Compute and store the training and test error for the regularized model\n",
    "    errors_outer['train']['regularized'][outer_fold_idx] = np.mean((y_train_outer - model.predict(X_train_outer))**2, axis=0)\n",
    "    errors_outer['test']['regularized'][outer_fold_idx] = np.mean((y_test_outer - model.predict(X_test_outer))**2, axis=0)\n",
    "\n",
    "\n",
    "    # Create and fit a model without regularization on the entire outer training set, for comparison\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_outer, y_train_outer)\n",
    "\n",
    "    # Store the model coefficients for the model without regularization\n",
    "    ws_outer['not regularized'][:, outer_fold_idx] = [model.intercept_] + model.coef_.flatten().tolist()\n",
    "    # Compute and store the training and test error for the model without regularization\n",
    "    errors_outer['train']['not regularized'][outer_fold_idx] = np.mean((y_train_outer - model.predict(X_train_outer))**2, axis=0)\n",
    "    errors_outer['test']['not regularized'][outer_fold_idx] = np.mean((y_test_outer - model.predict(X_test_outer))**2, axis=0)\n",
    "\n",
    "    # Compute mean squared error for the baseline, i.e. without using the input data at all\n",
    "    errors_outer['train']['baseline (no features)'][outer_fold_idx] = np.mean((y_train_outer - y_train_outer.mean())**2, axis=0)\n",
    "    errors_outer['test']['baseline (no features)'][outer_fold_idx] = np.mean((y_test_outer - y_test_outer.mean())**2, axis=0)\n",
    "\n",
    "# Print results\n",
    "print_regularization_results(errors_outer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
