{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bb31907",
   "metadata": {},
   "source": [
    "**02452** *Machine Learning*, Technical University of Denmark\n",
    "\n",
    "- This Jupyter notebook contains exercises where you fill in missing code related to the lecture topic. *First*, try solving each task yourself. *Then* use the provided solution (an HTML file you can open in any web browser) as inspiration if needed. If you get stuck, ask a TA for help.\n",
    "\n",
    "- Some tasks may be difficult or time-consuming - using the solution file or TA support is expected and perfectly fine, as long as you stay active and reflect on the solution.\n",
    "\n",
    "- You are not expected to finish everything during the session. Prepare by looking at the exercises *before* the class, consult the TAs *during* class, and complete the remaining parts *at home*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97115e4b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a3dfe8",
   "metadata": {},
   "source": [
    "# Week 9: Artificial Neural Networks and Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc73f00",
   "metadata": {},
   "source": [
    "**Content:**\n",
    "- Part 1: What is a neural network?\n",
    "- Part 2: A primer on gradient-based optimization\n",
    "- Part 3: Training a simple neural network \n",
    "- Part 4: Regression with a neural network\n",
    "- Part 5: Multinomial classification with a neural network\n",
    "\n",
    "**Objectives:**\n",
    "- Get a feeling of how artificial neural networks (ANN) can be used for data modelling and the role of hidden units in neural networks.\n",
    "- Understand how ANNs are trained using gradient-based optimization.\n",
    "- Understand how ANNs and multinomial regression (the generalization of logistic regression to multiple classes) can be applied to multi-class problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bcf653",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "# Plotting style\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_theme(font_scale=1.)\n",
    "\n",
    "def get_grid_points(x_min, x_max, y_min, y_max, delta=5e-3):\n",
    "    # Create a grid of points with the specified resolution\n",
    "    xx = np.arange(x_min, x_max, delta)\n",
    "    yy = np.arange(y_min, y_max, delta)\n",
    "    # Make a mesh-grid that spans the grid-range defined\n",
    "    grid = np.stack(np.meshgrid(xx, yy))\n",
    "    return grid, xx, yy\n",
    "\n",
    "def plot_decision_boundary(predict_function, X, threshold=None, fig=None, cmap='RdBu_r'):\n",
    "    # Set grid range based on the data\n",
    "    grid_range = [X[:, 0].min(), X[:, 0].max(), X[:, 1].min(), X[:, 1].max()]  # [x_min, x_max, y_min, y_max]\n",
    "    # Add 10% margin to the grid range to ensure points on the edge are included\n",
    "    margin_x = 0.1 * (grid_range[1] - grid_range[0])\n",
    "    margin_y = 0.1 * (grid_range[3] - grid_range[2])\n",
    "    grid_range[0] -= margin_x\n",
    "    grid_range[1] += margin_x\n",
    "    grid_range[2] -= margin_y\n",
    "    grid_range[3] += margin_y\n",
    "\n",
    "    # Get grid points\n",
    "    grid, xx, yy = get_grid_points(*grid_range, delta=5e-3)\n",
    "    # Reshape grid to a list of points\n",
    "    grid_points = torch.tensor(np.reshape(grid, (2, -1)).T, dtype=torch.float32)\n",
    "\n",
    "    # Compute model predictions on the grid points (i.e. the probability of class 1)\n",
    "    with torch.no_grad():  # No need to compute gradients for plotting\n",
    "        grid_predictions = predict_function(grid_points)\n",
    "\n",
    "    # Reshape the predictions back to the grid shape\n",
    "    decision_boundary = np.reshape(grid_predictions, (len(yy), len(xx)))\n",
    "\n",
    "    # Setup figure and axis for plotting\n",
    "    if fig is None:\n",
    "        fig = plt.figure(figsize=(6, 6))\n",
    "    ax = fig.gca()\n",
    "\n",
    "    # Plot the decision boundary\n",
    "    img = ax.imshow(decision_boundary, extent=grid_range, origin='lower', cmap=cmap, alpha=0.5)\n",
    "    fig.colorbar(img, ax=ax)\n",
    "    if threshold is not None:\n",
    "        ax.contour(grid[0], grid[1], decision_boundary, levels=[threshold], colors='k')\n",
    "    ax.grid(False)\n",
    "    ax.set_aspect('auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87632d83",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this week’s exercise, we will first introduce what a neural network is, then highlight why we need gradient-based optimization for training it. Later we will apply gradient-based optimization to train an ANN for a simple supervised learning task and finally explore applications of ANNs to real-world regression and multi-class classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53feedc0",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Part 1: What is a neural network?\n",
    "\n",
    "An artificial neural network is a flexible function that you can train to learn a specific task. You can think of it with an example as follows:\n",
    "\n",
    "You want to learn how to cook a good ramen (**train the network**) such that it matches the one from your favorite ramen restaurant (**the target function**). Initially, you randomly mix the ingredients (**input data**) by some amount of each ingredient (**the parameters/weights**), and you end up with a weird tasting dish (**the output**). You then compare it to the ramen from the restaurant (**compute the loss**) which gives you an idea on *how to adjust the amount of each ingredient* (**the gradient**) to make it taste the same. From this, you update your recipe and repeat the steps until the ramen has the right taste (**gradient descent**).\n",
    "\n",
    "In other words, a neural network is a flexible function, $f\\left(\\cdot, \\boldsymbol{w} \\right)$, that takes some input data $\\boldsymbol{x} \\in \\mathbb{R}^M$ and gives predictions $\\hat{\\boldsymbol{y}} = f(\\boldsymbol{x}, \\boldsymbol{w})$ depending on the setting of its parameters $\\boldsymbol{w}$. The \"recipe\" takes the form of a structured computational graph. In the figure below we show a 1-layer neural network that takes an input data point $\\boldsymbol{x}=\\begin{bmatrix}x_1, x_2 \\end{bmatrix}^\\top$ and makes a prediction $\\hat{y}$ through a hidden layer where the data is represented as $\\boldsymbol{z}^{(1)} = \\begin{bmatrix} z_1^{(1)}, z_2^{(1)} \\end{bmatrix}^\\top$.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"nn.png\" alt=\"Neural network example\" height=\"400px\">\n",
    "</p>\n",
    "\n",
    "Mathematically, a **neural network is a non-linear function** where the predicted output can be written as:\n",
    "$$\n",
    "    \\hat{y} = f\\left(\\boldsymbol{x}, \\boldsymbol{w}\\right) = h^{(2)} \\left( \\left(\\underset{=\\boldsymbol{z}^{(1)}}{\\underbrace{h^{(1)}\\left(\\boldsymbol{W}^{(1)} \\boldsymbol{x} + \\boldsymbol{b}\\right)}}\\right)^\\top \\boldsymbol{w}^{(2)} + b^{(2)} \\right)\n",
    "$$\n",
    "where we use the following notation:\n",
    "- $\\boldsymbol{w}=\\{\\boldsymbol{W}^{\\left(1\\right)}, \\boldsymbol{b}^{\\left(1\\right)}, \\boldsymbol{w}^{\\left(2\\right)}, b^{\\left(2\\right)}\\}$ is the collection of all weights and biases.\n",
    "- $\\boldsymbol{W}^{(1)} = \\begin{bmatrix} \\color{teal}{\\boldsymbol{w}_1^{(1)}}, \\color{orange}{\\boldsymbol{w}_2^{(1)}} \\end{bmatrix} = \\begin{bmatrix} \\color{teal}{w_{11}^{(1)}} & \\color{orange}{w_{21}^{(1)}} \\\\ \\color{teal}{w_{12}^{(1)}} & \\color{orange}{w_{22}^{(1)}} \\end{bmatrix}$ are the input layer weights and $\\boldsymbol{b}^{(1)}=\\begin{bmatrix} \\color{teal}{b_1^{(1)}} \\\\ \\color{orange}{b_2^{(1)}} \\end{bmatrix}$ are the input layer biases. \n",
    "- $\\color{violet}{\\boldsymbol{w}^{(2)}}=\\begin{bmatrix} w_1^{(2)} \\\\ w_2^{(2)}\\end{bmatrix}$ are the output layer weights and $\\color{violet}{b^{(2)}}$ is the output layer bias.\n",
    "- $h^{(1)}$ and $h^{(2)}$ are the *activation functions* (or transfer functions) for each layer, depicted above the nodes. Concretely, $h^{(1)}\\left(\\boldsymbol{x}\\right) := \\max\\left(0, \\boldsymbol{x}\\right)$ is the non-linear ReLU activation and $h^{(2)}$ is the linear function that does not change what is inputted to it. \n",
    "\n",
    "We chose to write the weights of each layer by $\\boldsymbol{W}^{(l)}$ where the superscript $l$ denotes the layer index. This is because we are free to increase the number of *neurons/hidden units* per layer as well as the number of layers when we define a neural network, which is exactly what makes the neural network highly flexible. In fact, the [universal approximation theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem) tells us that a 1-layer neural network with infinitely many hidden units can approximate any function! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1eb6e5",
   "metadata": {},
   "source": [
    "**Task 1.1:** How many parameters does the network have? How would this change if the hidden layer had 10 units?\n",
    "\n",
    "> *Hint:* First, you can simply count the number of weights and biases on the figure.\n",
    "\n",
    "> *Hint:* For the second question, think about how the weights for each input dimension changes, i.e. how many elements will $\\color{teal}{\\boldsymbol{w}_1^{(1)}}$ and $\\color{orange}{\\boldsymbol{w}_2^{(1)}}$ contain when increasing the number of units? What happens to $\\boldsymbol{b}^{\\left(1\\right)}$?\n",
    "\n",
    "> *Hint:* Remember to also consider what happens to $\\color{violet}{\\boldsymbol{w}^{(2)}}$. Does the dimensionality of $\\color{violet}{b^{\\left(2\\right)}}$ change?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998cf606",
   "metadata": {},
   "source": [
    "**Task 1.2:** Using pen and paper, compute $f_{\\boldsymbol{w}}\\left(\\boldsymbol{x}=\\begin{bmatrix} 1 \\\\ 1\\end{bmatrix}\\right)$, i.e. the predicted output of the above neural network. Use the following configurations:\n",
    "$$\n",
    "    \\color{teal}{\\boldsymbol{w}_1^{(1)}} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}, \\quad \\color{orange}{\\boldsymbol{w}_2^{(1)}} =  \\begin{bmatrix} 1 \\\\ 6 \\end{bmatrix}, \\quad \\color{violet}{\\boldsymbol{w}^{(2)}} = \\begin{bmatrix} -1 \\\\ 1 \\end{bmatrix} \n",
    "$$\n",
    "$$\n",
    "    b^{(1)} = \\begin{bmatrix} 1 \\\\ -2\\end{bmatrix}, \\quad b^{(2)} = 4\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061f00e2",
   "metadata": {},
   "source": [
    "**Task 1.3:** If there instead is only one unit in the hidden layer and in the output layer, and the activation function of each layer is linear, the neural network is the same as a simple model that we have seen previously in the course. Can you figure out which?\n",
    "\n",
    "> *Hint:* If you don't know how to start, consider drawing the network and adjust the function $f_{\\boldsymbol{w}}$ above accordingly.\n",
    "\n",
    "> *Hint:* The formula should be similar to something you have seen before. If you don't know where, check week 5.\n",
    "\n",
    "**Task 1.4:** What happens if you change the output activation function to the sigmoid function?\n",
    "\n",
    "> *Hint:* Repeat the steps from task 1.2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a57fd42",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Part 2: A primer on gradient-based optimization\n",
    "\n",
    "In machine learning, we often work with a data-dependent parametric model, which can be written mathematically as:\n",
    "$$\n",
    "f : \\mathbb{R}^M \\rightarrow \\mathbb{R}^D.\n",
    "$$\n",
    "Given a dataset of $\\left(\\boldsymbol{X}, \\boldsymbol{y}\\right)$, we can compute the loss $E(\\boldsymbol{X}, \\boldsymbol{y}, \\boldsymbol{w})$ that measures how well the model’s predictions $\\hat{\\boldsymbol{y}}=f\\left(\\boldsymbol{x}, \\boldsymbol{w}\\right)$ match the observed targets $\\boldsymbol{y}$. For finding the optimal model parameters, we solve an **optimization problem**:\n",
    "$$\n",
    "\\boldsymbol{w}^\\ast = \\arg \\min_{\\boldsymbol{w}} E(\\boldsymbol{X}, \\boldsymbol{y}, \\boldsymbol{w}),\n",
    "$$\n",
    "where $\\boldsymbol{w}^\\ast$ is the parameter setting that best aligns predictions with the observed data. In week 5 we solved the optimization problem for linear regression using the mean squared error (MSE) as the loss function. As linear regression is indeed a linear model, we were able to get a closed-form expression for the optimal model parameters. \n",
    "\n",
    "For neural networks, the optimization problem does not have a closed-form solution due to the non-linearities of the network. Instead we iteratively estimate the optimal parameters $\\boldsymbol{w}^\\ast$ using **gradient descent** which proceeds as follows:\n",
    "1. Initialize the parameter estimate as $\\boldsymbol{w}^{\\left(0\\right)}$ and choose the learning rate $\\alpha$\n",
    "2. At each time step $t\\in[1, \\dots T]$ we:\n",
    "    - compute the gradient of the loss wrt. $\\boldsymbol{w}$, i.e. $\\nabla_{\\boldsymbol{w}} E \\left(\\boldsymbol{X}, \\boldsymbol{y}, \\boldsymbol{w}^{\\left(t-1\\right)} \\right)$\n",
    "    - update the parameter estimate as $\\boldsymbol{w}^{\\left(t\\right)} \\leftarrow \\boldsymbol{w}^{\\left(t-1\\right)} - \\alpha \\cdot \\nabla_{\\boldsymbol{w}} E \\left(\\boldsymbol{X}, \\boldsymbol{y}, \\boldsymbol{w}^{\\left(t-1\\right)} \\right)$\n",
    "3. Use $\\boldsymbol{w}^\\ast = \\boldsymbol{w}_T$ as the optimal parameter estimate.\n",
    "\n",
    "Recall that the gradient is a vector with the partial derivatives of the function $E$, i.e. $\\nabla_{\\boldsymbol{w}} E = \\begin{bmatrix} \\frac{\\partial E}{\\partial w_1}, \\frac{\\partial E}{\\partial w_2}, \\dots, \\frac{\\partial E}{\\partial w_K} \\end{bmatrix}^\\top$. It describes the direction from $\\boldsymbol{w}$ where the loss function changes the most. In gradient descent we step-wise approach a minimum of the function by stepping opposite to the gradient where the learning rate $\\alpha$ controls the speed and stability of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae47505",
   "metadata": {},
   "source": [
    "**Task 2.1:** Consider the simple loss function that only depends on one parameter, i.e. $E\\left(w\\right) = w^4 - w^2 + \\frac{1}{4}w$. Derive an expression of the gradient using pen-and-paper.\n",
    "\n",
    "> *Hint:* You only have one parameter, so the gradient is just the derivative of $E$ wrt. $w$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb22501f",
   "metadata": {},
   "source": [
    "**Task 2.2:** Implement the gradient descent scheme from above and run it for $T=4$ steps with a learning rate of $\\alpha=0.01$. Start from $w_0=1.5$ and store the trajectory in a list.\n",
    "\n",
    "> *Hint:* Use a `for`-loop to run gradient descent for $T=4$ steps.\n",
    "\n",
    "> *Hint:* To store the parameter trajectory use `.append` on the list, e.g. `trajectory.append(wt)` where `wt` is the estimate at time $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5665d237",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4f2cdf71cf7837538c4134a0e8ec72a0",
     "grade": false,
     "grade_id": "cell-0774159ab390cc4a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "w0 = 1.5\n",
    "n_steps = 4\n",
    "learning_rate = 0.01\n",
    "\n",
    "trajectory = [w0]\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Define the loss function\n",
    "loss_function = lambda w: w**4 - w**2 + 1/4 * w \n",
    "\n",
    "# Compute the loss function values for a range of w values\n",
    "ws = np.linspace(-1.6, 1.6, 100)\n",
    "\n",
    "# Plot the loss function and the trajectory of gradient descent\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "plt.plot(ws, loss_function(ws), zorder=1)\n",
    "# Plot the trajectory points\n",
    "for t, w in enumerate(trajectory):\n",
    "    plt.plot(w, loss_function(w), 'ro', zorder=2)\n",
    "    if n_steps < 5 or t == 0 or t == n_steps:\n",
    "        plt.text(w - 0.15, loss_function(w) + 0.15, f'$w^{{({t})}}$', ha='center')\n",
    "\n",
    "plt.scatter(trajectory[-1], loss_function(trajectory[-1]), s=150, marker='*', color='yellow', edgecolors='k', label=r'$w^\\ast$', zorder=3)\n",
    "plt.title('Gradient descent trajectory')\n",
    "plt.xlabel(r'$w$')\n",
    "plt.ylabel(r'$E(w)$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0cb4fd",
   "metadata": {},
   "source": [
    "**Task 2.3:** Try to experiment with the number of steps by increasing it to $T=100$. How does this impact the estimate of $w^\\ast$? \n",
    "\n",
    "> *Hint:* Do you reach the local or the global minima?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de526c18",
   "metadata": {},
   "source": [
    "**Task 2.4:** Try to experiment with the learning rate $\\alpha$. What happens if you decrease it to $\\alpha=0.001$? Or increase it to $\\alpha=0.15$? For both cases, explain why.\n",
    "\n",
    "> *Hint:* In which of the cases do you reach the global minima instead of the local minima? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d699a24b",
   "metadata": {},
   "source": [
    "A common method for speeding up convergence as well as helping prevent getting stuck in local minima is to add *momentum* to the gradient descent update. The concept is similar to a ball rolling down a hill with some momentum from its previous location. Mathematically, gradient descent with momentum is:\n",
    "\n",
    "1. Initialize the parameter estimate as $\\boldsymbol{w}^{\\left(0\\right)}$ and update vector $\\boldsymbol{v}^{\\left(0\\right)}$. Choose the learning rate $\\alpha$.\n",
    "2. At each time step $t\\in[1, \\dots T]$ we:\n",
    "    - compute the gradient of the loss wrt. $\\boldsymbol{w}$, i.e. $\\nabla_{\\boldsymbol{w}} E \\left(\\boldsymbol{X}, \\boldsymbol{y}, \\boldsymbol{w}^{\\left(t-1\\right)} \\right)$\n",
    "    - compute the update with momentum as $ \\boldsymbol{v}^{\\left(t\\right)} = \\beta \\cdot \\boldsymbol{v}^{\\left(t-1\\right)} - \\alpha \\cdot \\nabla_{\\boldsymbol{w}} E \\left(\\boldsymbol{X}, \\boldsymbol{y}, \\boldsymbol{w}^{\\left(t-1\\right)} \\right)$\n",
    "    - update the parameter estimate as $\\boldsymbol{w}^{\\left(t\\right)} \\leftarrow \\boldsymbol{w}^{\\left(t-1\\right)} + \\boldsymbol{v}^{\\left(t\\right)}$\n",
    "3. Use $\\boldsymbol{w}^\\ast = \\boldsymbol{w}_T$ as the optimal parameter estimate.\n",
    "\n",
    "**Task 2.5 (Optional):** Implement the gradient descent with momentum. Set the momentum parameter to $\\beta=0.9$ and the learning rate to $\\alpha=0.01$. What happens to the size of the first 5 update steps? Is the final solution a global or local minima?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48a6883",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cea7d2a693bfa141677d1d9d012d6aeb",
     "grade": false,
     "grade_id": "cell-4c7df735575a4619",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "w0 = 1.5\n",
    "n_steps = 100\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "trajectory = [w0]\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Define the loss function\n",
    "loss_function = lambda w: w**4 - w**2 + 1/4 * w \n",
    "\n",
    "# Compute the loss function values for a range of w values\n",
    "ws = np.linspace(-1.6, 1.6, 100)\n",
    "\n",
    "# Plot the loss function and the trajectory of gradient descent\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "plt.plot(ws, loss_function(ws), zorder=1)\n",
    "# Plot the trajectory points\n",
    "for t, w in enumerate(trajectory):\n",
    "    plt.plot(w, loss_function(w), 'ro', zorder=2)\n",
    "    if n_steps < 5 or t == 0 or t == n_steps:\n",
    "        plt.text(w - 0.15, loss_function(w) + 0.15, f'$w^{{({t})}}$', ha='center')\n",
    "\n",
    "plt.scatter(trajectory[-1], loss_function(trajectory[-1]), s=150, marker='*', color='yellow', edgecolors='k', label=r'$w^\\ast$', zorder=3)\n",
    "plt.title('Gradient descent trajectory')\n",
    "plt.xlabel(r'$w$')\n",
    "plt.ylabel(r'$E(w)$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f93523a",
   "metadata": {},
   "source": [
    "In the above example, we considered a 1-dimensional parameter space so the gradient was just the scalar derivative. To complete the introduction to gradient descent, we now consider a function with a 2-dimensional parameter space where $\\boldsymbol{w} = \\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix}$. We choose the loss as $E(\\boldsymbol{w}) = \\boldsymbol{w}^\\top \\mathbf{\\Sigma} \\boldsymbol{w}$ where $\\mathbf{\\Sigma}=\\begin{bmatrix} 1.0 & 0.1 \\\\ 0.1 & 0.5 \\end{bmatrix}$.\n",
    "\n",
    "**Task 2.6:** Write out the loss explicitly as a function containing $w_1$ and $w_2$. Implement it as a loss function similarly to the previous exercise.\n",
    "\n",
    "> *Hint:* Write out the matrix-vector product.\n",
    "\n",
    "**Task 2.7:** Using pen and paper, write down the gradient which is now a vector. Adjust your gradient descent implementation to work with vectors in the 2D case.\n",
    "\n",
    "> *Hint:* Remember the definition of the gradient. Construct it as a vector using `np.array()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4312510",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "00e57db22f731aa7ddd53c611eb7fb7f",
     "grade": false,
     "grade_id": "cell-6acf052a0d31b2c7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Define a 2D loss function that depends on two coordinates\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Set the number of steps and learning rate\n",
    "num_steps = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Choose the initial point to start the gradient descent from\n",
    "w0 = np.array([1.5, -1.5])\n",
    "trajectory = [w0.flatten()]\n",
    "\n",
    "# Run gradient descent\n",
    "wt = w0\n",
    "for i in range(num_steps):\n",
    "    # Compute gradient of the loss function\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    wt = wt - learning_rate * grad\n",
    "    trajectory.append(wt.flatten())\n",
    "\n",
    "# Convert trajectory to a numpy array for easier indexing\n",
    "trajectory = np.vstack(trajectory)\n",
    "\n",
    "# Define the grid of points to evaluate the loss function for visualization\n",
    "xmin, xmax, ymin, ymax = -2, 2, -2, 2\n",
    "grid, xx, yy = get_grid_points(xmin, xmax, ymin, ymax, delta=5e-3)\n",
    "grid_points = np.reshape(grid, (2, -1)).T\n",
    "\n",
    "# Setup figure\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "# Plot the contour of the loss function\n",
    "loss = plt.contourf(loss_function(grid_points).reshape(len(yy), len(xx)), cmap='Blues_r', levels=15, extent=(xmin, xmax, ymin, ymax), origin='lower')\n",
    "# Plot the trajectory of gradient descent\n",
    "plt.plot(trajectory[:, 0], trajectory[:, 1], 'r-', label='Gradient descent path', zorder=2)\n",
    "# Highlight the starting and ending points\n",
    "plt.scatter(trajectory[0, 0], trajectory[0, 1], s=200, marker='.', color='red', edgecolors='k', label=r'$\\boldsymbol{w}^{(0)}$', zorder=3)\n",
    "plt.scatter(trajectory[-1, 0], trajectory[-1, 1], s=200, marker='*', color='yellow', edgecolors='k', label=r'$\\boldsymbol{w}^*$', zorder=3)\n",
    "plt.title('Gradient descent in 2D')\n",
    "plt.xlabel(r'$w_1$')\n",
    "plt.ylabel(r'$w_2$')\n",
    "plt.colorbar(loss, label='Loss function value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef1cf6f",
   "metadata": {},
   "source": [
    "We have now seen gradient descent for cases where we have few parameters and can visualize the loss function (also called the *loss landscape*). Even small neural networks like the one from Part 1 have more than 2 parameters which makes it impossible to visualize the loss landscape. However, the concept of gradient descent is exactly the same in neural networks where the parameter space is of higher than 2-dimensions. In the following, we will apply gradient descent to such networks without directly visualizing the gradient descent path in the parameter space. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8488f366",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Part 3: Training a simple neural network \n",
    "\n",
    "For constructing a neural network in practice, we will use an open source library called \"Pytorch\" (or `torch`). Pytorch supports running code on GPU (with CUDA) which can speed up computation time, however, we will only need to consider the CPU version in this course. If you are new to Pytorch, it can look complicated - so in the following exercises, we will do the setup for inspiration. \n",
    "\n",
    "We now have the basics covered and we can proceed with training a simple neural network similar to the one from Part 1. For doing so, we will consider an example that is highly visual, namely the `XOR` dataset. This dataset is made for binary classification.\n",
    "\n",
    "**Task 3.1:** Read through the code cell below. Which method do we use for splitting `X_train`, `y_train`, `X_test` and `y_test`? What is the dimensionality of the input and output space? \n",
    "\n",
    "> *Hint:* The input space dimensionality is the number of features. If in doubt, writing `X_train.shape` might help you.\n",
    "\n",
    "> *Hint:* For the output space dimensionality, use the same reasoning but for `y_train`.\n",
    "\n",
    "> *Hint:* We need to transform the data to `torch` tensors to use it in Pytorch. This is another datatype than `numpy` arrays that simply speaking are multi-dimensional matrices. The way they are defined within PyTorch enables e.g. processing on GPUs, which speeds up training of large networks. We can define the inputs to be of a specific float type by specyfing the datatype: `dtype=torch.float32`.\n",
    "\n",
    "> *Hint:* We use `torch.tensor(y_train).view(-1,1)` to make sure that the target is a column vector of shape $N\\times 1$. What is the shape of `y_train` if you remove `.view(-1,1)`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db3550e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # this is the Pytorch library\n",
    "\n",
    "# Load the XOR dataset\n",
    "df = pd.read_csv(\"data/xor.csv\")\n",
    "\n",
    "# Split the data into features and target\n",
    "X = df[['x1', 'x2']].values\n",
    "y = df['y'].values.astype(int)\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to torch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Define colors per class for plotting\n",
    "colors = ['tab:blue', 'tab:red']\n",
    "\n",
    "# Plot the XOR dataset\n",
    "plt.figure(figsize=(6, 6))\n",
    "for class_value in np.unique(y_train):\n",
    "    # Create a mask for the current class\n",
    "    mask = (y_train == class_value).flatten()\n",
    "    # Plot training points\n",
    "    plt.scatter(X_train[mask, 0], X_train[mask, 1], label=f'Class {int(class_value)} (train)', color=colors[int(class_value)])\n",
    "\n",
    "# Plot test points with a different marker (style)\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], edgecolor='k', facecolors='none', label=f'Test set')\n",
    "# Add labels and title\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.title('XOR Dataset')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9563dd",
   "metadata": {},
   "source": [
    "**Task 3.2:** Construct the neural network from Part 1. Define three variables: `input_dim` which is the number of attributes in the data, `hidden_dim` which is the number of nodes in the hidden layer and `output_dim` which is the dimensionality of the target attribute.\n",
    "\n",
    "> *Hint:* To define a neural network we use `torch.nn.Sequential`. Each argument to this function is a part of a sequentially applied network. We can get a layer that maps `M` features to `H` hidden units by using `torch.nn.Linear(M, H)`. We can then apply an activation function to the output of that layer by having the next argument be `torch.nn.ReLU()` or another activation function. To train a binary classifier, we need a suitable final activation function, which is for instance `torch.nn.Sigmoid()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c55c9d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1fb8ab1a76c2cc190fb1600a3b21c9cf",
     "grade": false,
     "grade_id": "cell-59dd754769ec303a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Define the neural network here. \n",
    "# We use define a function that returns a new instance of the model when called.\n",
    "# This is useful for creating multiple instances of the model with the same or different number of hidden units.\n",
    "def get_model(input_dim, hidden_dim, output_dim):\n",
    "    return torch.nn.Sequential(\n",
    "        torch.nn.Linear(in_features=input_dim, out_features=hidden_dim, bias=True),     # Input layer\n",
    "        torch.nn.ReLU(),                                                                # Activation function\n",
    "        torch.nn.Linear(in_features=hidden_dim, out_features=output_dim, bias=True),    # Output layer\n",
    "        torch.nn.Sigmoid(),                                                             # Output activation function (for binary classification)\n",
    "    )\n",
    "\n",
    "# Here we call a specific instance of the model\n",
    "model = get_model(input_dim, hidden_dim, output_dim)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80341de",
   "metadata": {},
   "source": [
    "Since we are solving a binary classification task, we used the `Sigmoid` activation as the output. This restricts the output to be between 0 and 1, hence describing the probability of belonging to class 1. \n",
    "\n",
    "Next, we want to train the neural network to solve the binary classification task. We first define the loss (or the loss *criterion*) as the *binary cross-entropy* which is common for binary classification ([see here for further details](https://www.geeksforgeeks.org/deep-learning/binary-cross-entropy-log-loss-for-binary-classification/)). Then, we define an optimizer (here, gradient descent) that tells us how to iteratively minimize the loss function by updating the model parameters. We do that in the *training loop*.\n",
    "\n",
    "**Task 3.3:** Read through the code below. Make sure you understand:\n",
    "1) how the loss criterion is defined and computed, \n",
    "2) how we define *what* to optimize, \n",
    "3) how we use the training and test data specifically, \n",
    "4) where we compute the *gradient*,\n",
    "5) how an *update* is made.\n",
    "\n",
    "> *Hint:* To iteratively update the model parameters, we set `params=model.parameters()` in the optimizer. \n",
    "\n",
    "> *Hint:* Computing the gradient of a neural network wrt. its parameters is not straight-forward by hand. Instead we use something called *back-propagation* that uses the chain rule to automatically compute the gradient. You are not expected to know the technical details but if you are interested, [this video gives a good introduction to backpropagation](https://www.youtube.com/watch?v=Ilg3gGewQ5U). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c366a13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a model instance with a specific number of hidden units\n",
    "model = get_model(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim)\n",
    "\n",
    "# Define loss criterion\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "# Define the learning rate for gradient descent\n",
    "lr = 0.05\n",
    "# Define the number of epochs (iterations over the entire dataset)\n",
    "n_epochs = 10000\n",
    "\n",
    "# Define the optimizer as stochastic gradient descent (SGD). \n",
    "# Here, the data is fixed, so it is equivalent to standard gradient descent.\n",
    "# You define the parameters to optimize (the model parameters) by setting params=model.parameters()\n",
    "optimizer = torch.optim.SGD(params=model.parameters(), lr=lr)\n",
    "\n",
    "# Define a dictionary to store the loss values for each epoch\n",
    "results = {'train': [], 'test': []}\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    # Make sure that the gradients are zero before you use backpropagation\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Make a forward pass through the model to compute the outputs\n",
    "    outputs = model(X_train)\n",
    "    # Compute the loss\n",
    "    loss = criterion(outputs, y_train)\n",
    "    # Do a backward pass to compute the gradients wrt. model parameters using backpropagation.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the model parameters by making the optimizer take a gradient descent step\n",
    "    optimizer.step()\n",
    "\n",
    "    with torch.no_grad(): # No need to compute gradients for the validation set\n",
    "        # Set the model to evaluation mode\n",
    "        model.eval()  \n",
    "        # Compute the loss for the test set\n",
    "        test_outputs = model(X_test)\n",
    "        test_loss = criterion(test_outputs, y_test)\n",
    "    \n",
    "    # Store the training and test loss for this epoch in the dictionary\n",
    "    results['train'].append(loss.item())\n",
    "    results['test'].append(test_loss.item())\n",
    "\n",
    "    # Print the loss every 1000 epochs\n",
    "    if (epoch+1) % 1000 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{n_epochs}], Loss: {loss.item():.4f}, Test Loss: {test_loss.item():.4f}')\n",
    "\n",
    "# Plot the optimization trace\n",
    "fig = plt.figure(figsize=(5, 4))\n",
    "plt.plot(results['train'], label='Training Loss')\n",
    "plt.plot(results['test'], label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Optimization trace')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b9e1eb",
   "metadata": {},
   "source": [
    "Now, we have a trained neural network that we can use for predicting the labels of the test set. Though we used the binary cross entropy as the loss criterion, it is hard to interpret - a better alternative is to consider the model accuracy.\n",
    "\n",
    "**Task 3.4:** Compute the predictions and accuracy of your model on the test set. Plot the predictions on top of the training data and print the accuracy score. How well does the model work on the test set and how can you see this from the decision boundary?\n",
    "\n",
    "> *Hint:* First, you need to pass the test data through your trained model by `test_outputs = model(X_test)`.\n",
    "\n",
    "> *Hint:* Since we used the sigmoid as the output activation, the outputs capture the probabilities of belonging to class 1. Use a threshold to convert the probability to class predictions, e.g. `y_pred = (test_outputs > 0.5).float()`.\n",
    "\n",
    "> *Hint:* Compute the accuracy by checking how large a fraction of the predictions that are equal to the true label. One idea is to do `(y_pred == y_test).float().mean()`\n",
    "\n",
    "> *Hint:* We provide you with a function for plotting the decision boundary that the model learned. It visualizes the probability of belonging to class 1.\n",
    "\n",
    "> *Hint:* To plot the predictions, take inspiration from what we do when plotting the training points. Consider using `edgecolor='k'` in `plt.scatter()` to be able to distinguish the predicted points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490fffed",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7177e855131e240edf2e193bc724fc6f",
     "grade": false,
     "grade_id": "cell-5494355e9552c00a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# COMPUTE PREDICTIONS AND ACCURACY ON THE TEST SET HERE\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Make a figure to plot the decision boundary and the data points\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "# Plot the decision boundary\n",
    "plot_decision_boundary(predict_function=model, X=X_train, threshold=0.5, fig=fig)\n",
    "\n",
    "for class_value in np.unique(y_train):\n",
    "    # Create a mask for the current class\n",
    "    mask = (y_train == class_value).flatten()\n",
    "    # Plot training points\n",
    "    plt.scatter(X_train[mask, 0], X_train[mask, 1], label=f'Class {int(class_value)} (train)', color=colors[int(class_value)])\n",
    "\n",
    "# PLOT THE PREDICTIONS HERE\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.title('XOR Dataset')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d15911",
   "metadata": {},
   "source": [
    "We can easily check the values of the optimal parameters found when running gradient descent above. We print these below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c17bb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_parameters = torch.cat([p.data.flatten() for p in model.parameters()])\n",
    "\n",
    "print(\"Optimal parameters found by gradient descent:\")\n",
    "print(all_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab271ec2",
   "metadata": {},
   "source": [
    "Note that you will get quite different results every time you run the script, since the loss function is *non-convex* and your results therefore depend on how the network was initialized (i.e. the random seed).\n",
    "\n",
    "**Task 3.5:** Re-run the previous code cells and verify that the results change. \n",
    "\n",
    "> *Hint:* You should be able to see this on the optimization trace figure of the training and test loss, the decision boundary as well as on the optimal parameters that we just printed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c7b622",
   "metadata": {},
   "source": [
    "From repeating the experiment, you should see that the model with 2 hidden units often only obtains a test set accuracy of approximately 75% and that the decision boundary looks linear in many cases. \n",
    "\n",
    "**Task 3.6:** Go back and retrain a more flexible network by increasing the number of units in the hidden layer to 3 and 100. Argue why this gives a better model. What are the drawbacks of increasing the number of hidden units?\n",
    "\n",
    "> *Hint:* Remember, that we can approximate *any* function by increasing the number of hidden units (universal approximation theorem).\n",
    "\n",
    "- *Answer:*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211e2dcf",
   "metadata": {},
   "source": [
    "**Task 3.8 (Optional):** What is the minimum number of hidden units that gives you a 100% accuracy on the test set? Can you explain why?\n",
    "\n",
    "> *Hint:* Think about how many linear boundaries are needed to split the 2D plane into regions that separate the XOR data correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ac0f5a",
   "metadata": {},
   "source": [
    "Usually, we use crossvalidation to optimize hyperparameters (i.e. the learning rate and number of hidden units). You have seen how to do this in previous weeks and the concept is exactly the same here - the only change is the model type and what the hyperparameters are."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5c16f8",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent\n",
    "\n",
    "You might have remarked that the optimizer we used before was called `torch.optim.SGD`, where the S in fact stands for *stochastic*. In the previous example, we however considered a fixed loss function and indeed nothing was stochastic in that setting.\n",
    "\n",
    "Stochastic gradient descent can be seen as an approximation of gradient descent - it replaces the actual gradient with an estimate of it. Specifically, remember that the loss is dependent on the specific dataset considered. Where normal gradient descent uses the entire data set to compute the loss and the gradient hereoff, stochastic gradient descent uses randomly selected subsets of the data to compute the gradient estimate. In high-dimensional optimization problems (e.g. large neural networks), this reduces the computational burden. \n",
    "\n",
    "**Task 3.9 (Optional):** Read through the code below. Make sure you understand where and how we compute the gradient estimate. What happens to the training loss?\n",
    "\n",
    "> *Hint:* The training loss curve is not smooth anymore. Why do you think that is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f41cd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a model instance with a specific number of hidden units\n",
    "model = get_model(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim)\n",
    "\n",
    "# Define loss criterion\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "# Define the learning rate for gradient descent\n",
    "lr = 0.05\n",
    "# Define the number of epochs (iterations over the entire dataset)\n",
    "n_epochs = 1000\n",
    "\n",
    "# Define batch size and number of batches\n",
    "N_train = X_train.shape[0]\n",
    "batch_size = 32\n",
    "num_batches = N_train // batch_size\n",
    "\n",
    "# Define the optimizer as stochastic gradient descent (SGD). \n",
    "optimizer = torch.optim.SGD(params=model.parameters(), lr=lr)\n",
    "\n",
    "# Define a dictionary to store the loss values for each epoch\n",
    "results = {'train': [], 'test': []}\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # Shuffle the training data for doing stochastic gradient descent\n",
    "    permutation_order = np.random.choice(np.arange(N_train), N_train, replace=False)                    \n",
    "    # Loop over smaller batches of data\n",
    "    for batch_idx in range(num_batches):\n",
    "        X_train_batch = X_train[batch_idx*batch_size:(batch_idx+1)*batch_size, :]\n",
    "        y_train_batch = y_train[batch_idx*batch_size:(batch_idx+1)*batch_size, :]\n",
    "\n",
    "        # Make sure that the gradients are zero before you use backpropagation\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make a forward pass through the model to compute the outputs\n",
    "        outputs = model(X_train_batch)\n",
    "        # Compute the loss for the batch\n",
    "        loss = criterion(outputs, y_train_batch)\n",
    "        # Do a backward pass to compute the gradient estimate wrt. model parameters using backpropagation.\n",
    "        loss.backward()\n",
    "        # Update the model parameters by making the optimizer take a gradient descent step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store the training loss for this epoch in the dictionary\n",
    "        results['train'].append(loss.item())\n",
    "\n",
    "    with torch.no_grad(): # No need to compute gradients for the validation set\n",
    "        # Set the model to evaluation mode\n",
    "        model.eval()  \n",
    "        # Compute the loss for the test set\n",
    "        test_outputs = model(X_test)\n",
    "        test_loss = criterion(test_outputs, y_test)\n",
    "        # Store the test loss for this epoch in the dictionary\n",
    "        results['test'].append(test_loss.item())\n",
    "\n",
    "    # Print the loss every 1000 epochs\n",
    "    if (epoch+1) % 1000 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{n_epochs}], Loss: {loss.item():.4f}, Test Loss: {test_loss.item():.4f}')\n",
    "\n",
    "# Plot the optimization trace\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "axs[0].plot(results['train'], label='Training Loss')\n",
    "axs[0].set_xlabel('Epoch')\n",
    "axs[0].set_ylabel('Loss')\n",
    "axs[0].set_title('Training Loss')\n",
    "\n",
    "axs[1].plot(results['test'], label='Test Loss', color='orange')\n",
    "axs[1].set_xlabel('Epoch')\n",
    "axs[1].set_ylabel('Loss')\n",
    "axs[1].set_title('Test Loss')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7b3dc7",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Part 4: Regression with a neural network\n",
    "\n",
    "We can use ANNs for both classification and regression. When we train an ANN to do regression, we do not a apply a tranfer function to the output node(s), and we train the network using a mean-square-error loss. In the following, we will again consider the Wine dataset that we have worked with in previous weeks.\n",
    "\n",
    "In the cell below, we load the data and construct the data matrix and target attribute. We will try to predict the alcohol content of each wine using a neural network for regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9c7fae",
   "metadata": {},
   "source": [
    "**Task 4.1:** Define the in- and output dimensions for the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3011a342",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6ac87eeb36871618da6595d3dbd8fa53",
     "grade": false,
     "grade_id": "cell-5a24166a5c54204f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/wine.csv\")\n",
    "\n",
    "# Split the data into features and target\n",
    "X = df.drop(columns=['Alcohol']).values\n",
    "y = df['Alcohol'].values.reshape(-1, 1)\n",
    "\n",
    "# Dimensionality of data\n",
    "N, M = X.shape\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5626e91e",
   "metadata": {},
   "source": [
    "**Task 4.2:** Split the data into a test set and a training set using the holdout method such that 20% of the data is in the test set.\n",
    "\n",
    "> *Hint:* Use `X, X_test, y, y_test = train_test_split(X, y, test_size=test_size, random_state=42)`. This definition of variables is important for the coming code cells to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95e3dcd",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "72701ff5be4fae5ab3c67e580c1f6ec6",
     "grade": false,
     "grade_id": "cell-010afd117527ced0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f132c8",
   "metadata": {},
   "source": [
    "**Task 4.3:** Define a one-layer neural network for regression in the function `get_model()`.\n",
    "\n",
    "> *Hint:* What is the difference between a classification and regression model? You only need to remove one line of code from the previously defined function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98ea3de",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "599941e40ca3e1a305ef56072eba26cc",
     "grade": false,
     "grade_id": "cell-2d14a0ae95be634c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_model(input_dim, hidden_dim, output_dim):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc156351",
   "metadata": {},
   "source": [
    "In the cell below, we provide a framework for training a neural network and doing hyperparameter optimization on the number of hidden units using one-level cross-validation. We leave some critical parts for you.\n",
    "\n",
    "**Task 4.4:** Standardize the data within each cross-validation fold. Why is it important to do it within each fold?\n",
    "\n",
    "> *Hint:* Remember that you have to subtract the mean and divide by the standard deviation for each attribute when doing standardization.\n",
    "\n",
    "**Task 4.5:** Define the loss criterion.\n",
    "\n",
    "> *Hint:* Remember that we usually use the mean squared error as the criterion for regression problems. This can be implemented as `torch.nn.MSELoss()`.\n",
    "\n",
    "**Task 4.6:** Implement the training loop.\n",
    "\n",
    "> *Hint:* Take inspiration from previous tasks.\n",
    "\n",
    "> *Hint:* It is up to you whether you want to do gradient descent or stochastic gradient descent with batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3277d5f8",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "63a2b3ad7d1dcb4a799291723d8da13e",
     "grade": false,
     "grade_id": "cell-257f47e8a90388c1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# K-fold crossvalidation\n",
    "K = 3\n",
    "CV = KFold(K, shuffle=True, random_state=0)\n",
    "\n",
    "# Define hyperparameters\n",
    "lr = 1e-3\n",
    "n_epochs = 1000\n",
    "\n",
    "# Seed for reproducibility\n",
    "seed = 0\n",
    "\n",
    "# Hyperparameter tuning loop with K-fold crossvalidation\n",
    "hyperparameters_to_tune = [1, 2, 10, 50]\n",
    "\n",
    "results = {}\n",
    "for k, (train_index, val_index) in enumerate(CV.split(X, y)):\n",
    "    print(f'Fold {k+1}/{K}')\n",
    "    \n",
    "    # Get the training and test data for this fold\n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "    # Normalize data here\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    # Convert to torch tensors\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "    X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "    y_val = torch.tensor(y_val, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    # Set up a dictionary to store the results for each hyperparameter setting\n",
    "    results_inner = {hidden_dim: {'train': [], 'val': []} for hidden_dim in hyperparameters_to_tune}\n",
    "\n",
    "    # Loop over the hyperparameter settings        \n",
    "    for hidden_dim in hyperparameters_to_tune:\n",
    "        # Define a model instance with a specific number of hidden units\n",
    "        model = get_model(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim)\n",
    "\n",
    "        # Define loss criterion\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        # Define the optimizer as the Adam optimizer (not needed to know the details)\n",
    "        optimizer = torch.optim.SGD(params=model.parameters(), lr=lr)\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            # Implement the training loop here\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "\n",
    "            # Store the training loss for this epoch in the dictionary\n",
    "            results_inner[hidden_dim]['train'].append(loss.item())\n",
    "\n",
    "        # Compute the final test loss on the test set\n",
    "        with torch.no_grad(): # No need to compute gradients for the validation set\n",
    "            model.eval()\n",
    "            val_outputs = model(X_val)\n",
    "            val_loss = criterion(val_outputs, y_val)\n",
    "            results_inner[hidden_dim]['val'].append(val_loss.item())\n",
    "            print(f'  Hidden units: {hidden_dim}, Validation set MSE: {val_loss.item():.4f}')\n",
    "\n",
    "    # Store the results for this fold\n",
    "    results[k] = results_inner\n",
    "\n",
    "\n",
    "# Plot the loss curves for each fold and hyperparameter setting\n",
    "fig, axs = plt.subplots(1, K, figsize=(12, 4), sharey=True, sharex=True)\n",
    "# Plot the training loss for each fold and hyperparameter setting\n",
    "for fold in range(K):\n",
    "    for hidden_dim in hyperparameters_to_tune:\n",
    "        # Plot the training loss for this hyperparameter setting\n",
    "        axs[fold].plot(results[fold][hidden_dim]['train'], label=f'hidden_dim={hidden_dim}')\n",
    "\n",
    "    # Set the title and labels for each subplot\n",
    "    axs[fold].set_title('Fold {}'.format(fold+1))\n",
    "    axs[fold].set_xlabel('Epoch')\n",
    "    axs[fold].set_ylabel('MSE')\n",
    "\n",
    "# Set the overall title and show the legend\n",
    "plt.suptitle('Training loss for different hidden units')\n",
    "plt.tight_layout()\n",
    "axs[0].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a5e332",
   "metadata": {},
   "source": [
    "**Task 4.7:** How many hidden units would you choose based on the results? Justify your answer.\n",
    "\n",
    "> *Hint:* What are the pros and cons of increasing / decreasing the number of hidden units? How few number of units do you need to be able to interpret how the model uses the specific attributes in its output?\n",
    "\n",
    "- *Answer:*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75cc8f5",
   "metadata": {},
   "source": [
    "**Task 4.8:** We used gradient descent for optimizing the model. What happens to the results if you include momentum with $\\beta=0.9$ to the optimizer?\n",
    "\n",
    "> *Hint:* Add `momentum=0.9` as an input to the `torch.optim.SGD`-function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b8c932",
   "metadata": {},
   "source": [
    "When dealing with regression outputs, a simple way of looking at the quality of predictions visually is by plotting the estimated value as a function of the true/known value - these values should all be along a straight line \"y=x\", and if the points are above the line, the model overestimates, whereas if the points are below the y=x line, then the model underestimates the value\n",
    "\n",
    "**Task 4.9:** Train a neural network with the optimal number of hidden units on all data except the held out part. Evaluate the performance of the model on the held out set `X_test` by computing the MSE and by plotting the predictions against the true targets `y_test`.\n",
    "\n",
    "> *Hint:* Remember to do the same steps as we did above, i.e. normalization etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d998e62",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9de9a25fc82c299747c0630129a064ec",
     "grade": false,
     "grade_id": "cell-3352be766719e3a5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "y_est = test_outputs.data.numpy()\n",
    "y_true = y_test.data.numpy()\n",
    "axis_range = [np.min([y_est, y_true]) - 1, np.max([y_est, y_true]) + 1]\n",
    "plt.plot(axis_range, axis_range, \"k--\")\n",
    "plt.plot(y_true, y_est, \"ob\", alpha=0.25)\n",
    "plt.legend([\"Perfect estimation\", \"Model estimations\"])\n",
    "plt.title(\"Alcohol content: estimated versus true value (for last CV-fold)\")\n",
    "plt.ylim(axis_range)\n",
    "plt.xlim(axis_range)\n",
    "plt.xlabel(\"True value\")\n",
    "plt.ylabel(\"Estimated value\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692a7462",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Part 5: Multinomial classification with a neural network\n",
    "\n",
    "We have previously used naïve Bayes, K-nearest neighbors and decision trees for the classification of data with multiple classes. Logistic regression and artificial neural networks (ANN) can however also be extended to multiple classes by use of the softmax function given by $f_c\\left(\\boldsymbol{o}\\right) = \\frac{\\exp \\left(\\boldsymbol{o}_c\\right)}{\\sum_{c^\\prime}\\exp \\left(\\boldsymbol{o}_{c^\\prime}\\right)}$. Thus, \\boldsymbol{o}_c corresponds to the predictions made for the $c$'th class and all predictions are tied together through the softmax function. When training the model such that the output of the function $f_c\\left(\\boldsymbol{o}\\right)$ can be interpreted as the probability that the observation belongs to the $c$'th class. The softmax link function is for two class problems equivalent to the logit link function (the inverse sigmoid) and this particular extension of logistic regression to multi-class is called **multinomial regression**.\n",
    "\n",
    "In the following we will use a neural network for multinomial classification on the synthetic dataset considered in week 2. Below we load the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1aa91f2",
   "metadata": {},
   "source": [
    "**Task 5.1:** Convert the training and test dataset to `torch` tensors.\n",
    "\n",
    "> *Hint:* For inspiration, see previous tasks.\n",
    "\n",
    "> *Hint:* We need to use the datatype `torch.long` for the targets when doing multiclass classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09df337",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "569297e06a6087153bea14bb68a7e77d",
     "grade": false,
     "grade_id": "cell-5574a47d35306504",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "dataset_name = 'synth1'\n",
    "\n",
    "# Load dataset splits\n",
    "df_train = pd.read_csv(f\"data/synth/{dataset_name}_train.csv\")\n",
    "df_test = pd.read_csv(f\"data/synth/{dataset_name}_test.csv\")\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X_train = df_train[['x0', 'x1']].values\n",
    "y_train = df_train['y'].values.astype(int)\n",
    "X_test = df_test[['x0', 'x1']].values\n",
    "y_test = df_test['y'].values.astype(int)\n",
    "\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red']\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "for class_value in np.unique(y_train):\n",
    "    # Create a mask for the current class\n",
    "    mask = (y_train == class_value)\n",
    "    # Plot training points\n",
    "    plt.scatter(X_train[mask, 0], X_train[mask, 1], label=f'Class {int(class_value)}', color=colors[int(class_value)])\n",
    "\n",
    "# Plot the test points with a different marker (style)\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], edgecolor='k', facecolors='none', label=f'Test set')\n",
    "# Add labels and title\n",
    "plt.xlabel('$x_0$')\n",
    "plt.ylabel('$x_1$')\n",
    "\n",
    "plt.title('Synthetic dataset')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Convert to torch tensors\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2812f87c",
   "metadata": {},
   "source": [
    "**Task 5.2:** Define a 1-layer neural network that is suitable for multinomial regression.\n",
    "\n",
    "> *Hint:* Again, all we need to do is change the output activation in the `get_model` function.\n",
    "\n",
    "> *Hint:* The softmax activation function is defined as `torch.nn.Softmax(dim=1)`. We specify `dim=1` - if we had used `dim=0`, the function would have\n",
    "normalized not over classes but over the number of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cfcc1e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c5b6203dfb876578dc58710a63cd2249",
     "grade": false,
     "grade_id": "cell-65f31037c016a428",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_model(input_dim, hidden_dim, output_dim):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a132f2",
   "metadata": {},
   "source": [
    "When doing multi-class classification, in addition to using the softmax as the output activation function, we also need to use the **cross entropy loss**.\n",
    "\n",
    "**Task 5.3:** Implement the training loop to train a 1-layer neural network for classification with $5$ hidden units on the training data. Choose a suitable learning rate and number of epochs.\n",
    "\n",
    "> *Hint:* Define the input and output dimensionality. What is the output dimensionality when we are classifying between 4 classes?\n",
    "\n",
    "> *Hint:* The cross entropy loss is implemented as `torch.nn.CrossEntropyLoss()`.\n",
    "\n",
    "> *Hint:* Compute the loss and accuracy for the training and test sets every epoch. Store the results like we did previously. We plot the results in the end of the cell.\n",
    "\n",
    "> *Hint:* The `outputs` are the class probabilities. To get the predictions we can use `y_test_pred = torch.argmax(test_outputs, dim=1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795f2be1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0a651fd120995db7037bab763a8746ff",
     "grade": false,
     "grade_id": "cell-5e88f288e2ff487d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Seed for reproducibility\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Set up the model, criterion, optimizer, and results storage\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Plot the optimization trace\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "axs[0].plot(results['train loss'], label='Training Loss')\n",
    "axs[0].plot(results['test loss'], label='Test Loss')\n",
    "axs[0].set_xlabel('Epoch')\n",
    "axs[0].set_ylabel('Loss')\n",
    "axs[0].set_title('Loss trace')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(results['train accuracy'], label='Training Accuracy')\n",
    "axs[1].plot(results['test accuracy'], label='Test Accuracy')\n",
    "axs[1].set_xlabel('Epoch')\n",
    "axs[1].set_ylabel('Accuracy')\n",
    "axs[1].set_title('Accuracy trace')\n",
    "axs[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312f23aa",
   "metadata": {},
   "source": [
    " We plot the decision boundary below. \n",
    " \n",
    " **Task 5.4:** Use the trained model to predict on the test data and plot the predictions on top of the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9900d8d8",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "86179994300cdf8b91a0413f3f3e6087",
     "grade": false,
     "grade_id": "cell-f2f982c2e6a6293a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 6))\n",
    "plot_decision_boundary(predict_function=lambda x: torch.argmax(model(x), dim=1), X=X_train, fig=fig)\n",
    "\n",
    "for class_value in np.unique(y_train):\n",
    "    # Create a mask for the current class\n",
    "    mask = (y_train == class_value).flatten()\n",
    "    # Plot training points\n",
    "    plt.scatter(X_train[mask, 0], X_train[mask, 1], label=f'Class {int(class_value)} (train)', color=colors[int(class_value)])\n",
    "\n",
    "# PLOT THE PREDICTIONS HERE\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('$x_0$')\n",
    "plt.ylabel('$x_1$')\n",
    "plt.title('Synthetic dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dd12da",
   "metadata": {},
   "source": [
    "**Task 5.5:** Go back and re-run the experiment on the `synth2` dataset. What happens to the fit? Experiment with the learning rate and number of hidden units to find a good model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e062b9d",
   "metadata": {},
   "source": [
    "**Task 5.6 (Optional):** Fit a multinomial regression model to the dataset. Plot the predictions, decision boundary and compare it to the neural network.\n",
    "\n",
    "> *Hint:* Multinomial regression models are based on tieing the outputs by the softmax function. Especially consider the `synth3` dataset. Notice the multinomial regression model is able to solve this problem; look at the weights and explain how this is accomplished (n.b. notice the vertical dimension does not matter).\n",
    "\n",
    "> *Hint:* Use `model = sklearn.linear_model.LogisticRegression(multi_class=\"multinomial\", max_iter=1000, random_state=0)` to define the model.\n",
    "\n",
    "> *Hint:* Display coefficients by using `print(model.coef_)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d502a2",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f549022b9c0e16d9f21723b39ed3fe83",
     "grade": false,
     "grade_id": "cell-a612df5cbe24732a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
