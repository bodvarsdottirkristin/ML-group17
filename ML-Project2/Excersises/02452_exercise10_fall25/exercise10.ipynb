{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d182743",
   "metadata": {},
   "source": [
    "**02452** *Machine Learning*, Technical University of Denmark\n",
    "\n",
    "- This Jupyter notebook contains exercises where you fill in missing code related to the lecture topic. *First*, try solving each task yourself. *Then* use the provided solution (an HTML file you can open in any web browser) as inspiration if needed. If you get stuck, ask a TA for help.\n",
    "\n",
    "- Some tasks may be difficult or time-consuming - using the solution file or TA support is expected and perfectly fine, as long as you stay active and reflect on the solution.\n",
    "\n",
    "- You are not expected to finish everything during the session. Prepare by looking at the exercises *before* the class, consult the TAs *during* class, and complete the remaining parts *at home*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eee3d72",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcd2a7b",
   "metadata": {},
   "source": [
    "# Week 10: \tK-means and hierarchical clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43276f5d",
   "metadata": {},
   "source": [
    "**Content:**\n",
    "- Part 1: K-means clustering\n",
    "- Part 2: Cluster validity measures\n",
    "- Part 3: K-means as data compression\n",
    "- Part 4: Hierarchical Clustering\n",
    "- Part 5: Clustering on the Old Faithful geyser dataset\n",
    "\n",
    "**Objectives:**\n",
    "- Understand the difference between supervised and unsupervised learning\n",
    "- Understand the principles behind K-means and hierarchical clustering\n",
    "- Understand how different linkage functions affects clustering types\n",
    "- Compare clustering solutions using Rand index, Jaccard and NMI\n",
    "- Evaluate clustering quality using class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8365fc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "# Algorithms\n",
    "from sklearn.cluster import k_means\n",
    "from scipy.cluster.hierarchy import dendrogram, fcluster, linkage\n",
    "from sklearn.metrics import rand_score, normalized_mutual_info_score\n",
    "\n",
    "# Plotting style\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_theme(font_scale=1.)\n",
    "\n",
    "# Helper functions\n",
    "def gauss_2d(centroid, ccov, std=2, points=100):\n",
    "    \"\"\"\n",
    "    Returns two vectors representing slice through gaussian, cut at given standard deviation.\n",
    "\n",
    "    Parameters:\n",
    "    centroid (array-like): The centroid of the Gaussian distribution.\n",
    "    ccov (array-like): The covariance matrix of the Gaussian distribution.\n",
    "    std (float, optional): The standard deviation at which to cut the Gaussian distribution. Default is 2.\n",
    "    points (int, optional): The number of points to sample along the slice. Default is 100.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing two vectors representing the slice through the Gaussian distribution.\n",
    "    \"\"\"\n",
    "    mean = np.c_[centroid]\n",
    "    tt = np.c_[np.linspace(0, 2 * np.pi, points)]\n",
    "    x = np.cos(tt)\n",
    "    y = np.sin(tt)\n",
    "    ap = np.concatenate((x, y), axis=1).T\n",
    "    d, v = np.linalg.eig(ccov)\n",
    "    d = std * np.sqrt(np.diag(d))\n",
    "    bp = np.dot(v, np.dot(d, ap)) + np.tile(mean, (1, ap.shape[1]))\n",
    "    return bp[0, :], bp[1, :]\n",
    "\n",
    "def clusterplot(X, clusterid=None, centroids=None, y=None, covariances=None):\n",
    "    \n",
    "    if clusterid is not None:\n",
    "        for i, pred_class in enumerate(np.unique(clusterid)):\n",
    "            mask = (clusterid == pred_class)\n",
    "            plt.scatter(X[mask, 0], X[mask, 1], s=100, facecolors=f'C{i}', label=f'Predicted class {pred_class}', linewidth=5)\n",
    "\n",
    "    for i, true_class in enumerate(np.unique(y)):\n",
    "        mask = (y == true_class)\n",
    "        plt.scatter(X[mask, 0], X[mask, 1], s=50, c=f\"C{i}\", edgecolors='k', label=f'True class {true_class}')\n",
    "\n",
    "    if centroids is not None:\n",
    "        plt.scatter(centroids[:, 0], centroids[:, 1], c='red', s=100, marker='X')\n",
    "\n",
    "    if covariances is not None:\n",
    "        for i in range(len(centroids)):\n",
    "            x1, x2 = gauss_2d(centroids[i], covariances[i])\n",
    "            plt.plot(x1, x2, \"-\", color=f'C{i}', linewidth=3)\n",
    "\n",
    "    plt.title(\"Cluster Plot\")\n",
    "    plt.xlabel(\"Feature 1\")\n",
    "    plt.ylabel(\"Feature 2\")\n",
    "    plt.legend(ncols=2)\n",
    "\n",
    "def show_image(vec, resolution):\n",
    "    img = vec.reshape(resolution)\n",
    "    if resolution[0] == 1:\n",
    "        img = np.squeeze(img)\n",
    "        plt.imshow(img, cmap=\"gray\")\n",
    "    else:\n",
    "        plt.imshow(img.T)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "def plot_compression(X, centroids, cluster_ids, resolution):\n",
    "    # Initialize plot\n",
    "    K = len(centroids)\n",
    "    n_show = 5\n",
    "    n_cols = 5\n",
    "    n_rows = int(np.ceil(K / n_cols))\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(2.5 * n_cols, 2.5 * n_rows))\n",
    "    fig.suptitle(\"Centroids\", fontsize=16, weight=\"bold\", y=1.02)\n",
    "\n",
    "    # Plot centroids\n",
    "    axes = axes.ravel()\n",
    "    for i in range(K):\n",
    "        plt.sca(axes[i])\n",
    "        show_image(centroids[i], resolution)\n",
    "        plt.title(f\"Centroid {i}\", fontsize=10)\n",
    "\n",
    "    # Turn off any unused axes\n",
    "    for j in range(K, len(axes)):\n",
    "        axes[j].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot random faces and their centroids\n",
    "    idxs = np.random.choice(len(cluster_ids), n_show, replace=False)\n",
    "    fig, axes = plt.subplots(2, n_show, figsize=(2.5 * n_show, 5))\n",
    "    fig.suptitle(\"Example faces and their nearest centroids\", fontsize=16, weight=\"bold\", y=1.02)\n",
    "\n",
    "    for i, idx in enumerate(idxs):\n",
    "        # Original image\n",
    "        plt.sca(axes[0, i])\n",
    "        show_image(X[idx], resolution)\n",
    "        plt.title(f\"Face #{idx}\", fontsize=9)\n",
    "\n",
    "        # Corresponding centroid\n",
    "        plt.sca(axes[1, i])\n",
    "        show_image(centroids[cluster_ids[idx]], resolution)\n",
    "        plt.title(f\"Centroid {cluster_ids[idx]}\", fontsize=9)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f1680f",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Introduction:\n",
    "In previous exercises, we considered supervised learning, where we were provided with both input data $\\boldsymbol{X}$ and corresponding target outputs $\\boldsymbol{y}$. In classification tasks, these outputs were discrete labels, while in regression they were continuous values. The goal in supervised learning was to learn a mapping from $\\boldsymbol{X}$ to $\\boldsymbol{y}$ that generalizes well to unseen data.\n",
    "\n",
    "We now move on to unsupervised learning, where only the input data $\\boldsymbol{X}$ is available. Here, we do not have target outputs to guide the learning process. Instead, the goal is to uncover hidden structure within the data - for example, by identifying groups of observations that share similar characteristics. This process is known as clustering.\n",
    "\n",
    "We will explore two fundamental clustering methods: k-means clustering and hierarchical clustering. Through these exercises, you will gain a deeper understanding of how unsupervised learning methods discover structure in data, how to interpret clustering results, and how to evaluate clustering quality both with and without reference labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa30a60b",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Part 1: K-means clustering\n",
    "\n",
    "In this part of the exercise we will investigate k-means clustering. \n",
    "\n",
    "In k-means, we start by initializing $K$ cluster centers. Each of the data points are assigned to the cluster in closest proximity, according to some measure of distance. When the distance is given by the squared Euclidean distance, the centers are commonly denoted centroids. \n",
    "\n",
    "Once the data points have been assigned to a cluster, we update each cluster center to be placed at the center of the data points that are assigned to that cluster. This continues iteratively, usually until the assignment of data points to centers no longer change or until a maximal number of iterations is reached. \n",
    "\n",
    "An example gif is shown below:\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; align-items: center; gap: 20px;\">\n",
    "  <img src=\"kmeans_illustration_loop.gif\" alt=\"K-means Illustration\" style=\"width: 25%;\">\n",
    "  <img src=\"kmeans_pseudocode.png\" alt=\"K-means Pseudocode\" style=\"width: 25%;\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04e4440",
   "metadata": {},
   "source": [
    "In the following, we will cluster a simple synthetic dataset, located in `data/synth1.csv` using the k-means algorithm.\n",
    "\n",
    "**Task 1.1:** Load the data into a dataframe, inspect it, and split it into $\\left(\\boldsymbol{X}, \\boldsymbol{y}\\right)$. \n",
    "\n",
    "**Task 1.2:** Use the `clusterplot(X=X, y=y)`-function to visualize the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6058404a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "78694c5768959d8a6ab1783ebb38d82b",
     "grade": false,
     "grade_id": "cell-b849db0f13116abe",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "dataset_name = \"synth1\"\n",
    "\n",
    "# 1.1) Load the data\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# 1.2) Visualize the data with clusterplot\n",
    "plt.figure(figsize=(8, 6))\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e231e7",
   "metadata": {},
   "source": [
    "**Task 1.3:** Considering the scattered points, how many centroids should we initialize our K-means algorithm with? Are there any points that seem like they might be difficult to assign correctly?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97b3447",
   "metadata": {},
   "source": [
    "We will now assign each point to a cluster using the K-means algorithm. \n",
    "\n",
    "**Task 1.4:** Run the K-means algorithm on the synthetic data, using the `k_means` function from `sklearn.cluster`.\n",
    "> *Hint:* Save the outputs as `centroids, cluster_ids, _`. The last output is not needed here, hence the `_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06013d74",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a3a549459f68b3d3594074dd63e0fa86",
     "grade": false,
     "grade_id": "cell-7dc200675929705c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Number of clusters:\n",
    "K = 4\n",
    "\n",
    "# K-means clustering:\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Plot results:\n",
    "plt.figure(figsize=(14, 9))\n",
    "clusterplot(X=X, clusterid=cluster_ids, centroids=centroids, y=y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460520fe",
   "metadata": {},
   "source": [
    "**Task 1.5:** Do the cluster assignments correspond to the original true classes? Does that matter?\n",
    "- *Answer:*\n",
    "> *Hint:* Think of what we're trying to do in terms of what you have learned about supervised / unsupervised methods. \n",
    "\n",
    "**Task 1.6:** Try running the above code-blocks on the other synthetic datasets, `synth2`, `synth3` and `synth4`. Why might some of these perform badly when using K-means with a euclidean distance metric?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e22c9d7",
   "metadata": {},
   "source": [
    "\n",
    "--- \n",
    "## Part 2: K-means as data compression (Optional)\n",
    "\n",
    "K-means clustering has many different applications, one of which is data compression. A data set can be compressed by performing k-means clustering and then representing each data object by its cluster center. Thus, the only data that need to be stored is the $K$ cluster centers and the $N$ cluster indices.\n",
    "\n",
    "We will now consider a subset of the wild faces dataset described [here](https://osf.io/6p4r7/overview). \n",
    "Each data object is a $40 \\times 40 \\times 3 = 4800$ dimensional vector, corresponding to an image of $40 \\times 40$ pixels, with $3$ color channels each. \n",
    "\n",
    "**Task 2.1:** Load the Wild Faces dataset from `data/wildfaces.csv`.\n",
    "> *Hint:* Extract the data into `X` by using `df.values`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d5ea2c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c1bcb6004d14c7876905a8da56ce3646",
     "grade": false,
     "grade_id": "cell-73c329245d0d238a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 2.1) Load the Wild Faces dataset from data/wildfaces.csv\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Image resolution\n",
    "resolution = (3, 40, 40) # (color-channels, height, width)\n",
    "\n",
    "# Show image example\n",
    "show_image(X[0], resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323c7287",
   "metadata": {},
   "source": [
    "The following code block computes a k-means clustering of the data with K = 10 clusters. Then plots the centroids as images, as well as 5 random images from the data set as well as their corresponding cluster centroids.\n",
    "\n",
    "**Task 2.2:** How well is the data represented by the cluster centroids? Are you able to recognize the faces in the compressed representation? What happens if you increase or decrease the number of clusters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b0dc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run K-means clustering\n",
    "K = 10\n",
    "centroids, cluster_ids, _ = k_means(X, K, verbose=False, max_iter=100, n_init=1)\n",
    "\n",
    "plot_compression(X, centroids, cluster_ids, resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba6f7ba",
   "metadata": {},
   "source": [
    "**Task 2.3:** Load the digits dataset instead (`data/digits.npy`).\n",
    "> *Hint:* The last column represents the label, `y`, you can disregard this when loading into `X`.\n",
    "\n",
    "> *Hint:* Each data object is a $16\\cdot 16=256$ dimensional vector, corresponding to a gray scale $16\\times 16$ pixels image - what is the `resolution` in this case?\n",
    "\n",
    "**Task 2.4:** Why does running k-means with $K=10$ not give you 10 clusters corresponding to the $10$ digits $0 \\dots 9$? How many clusters do you need to visually represent the 10 different digits? Are there any digits that the clustering algorithm seems to confuse more than others?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fea63e",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Part 3: Cluster validity measures\n",
    "\n",
    "Previously, we have evaluated our supervised learning methods in terms of error rate and accuracy. This however, requires that we can match the estimated outcome to the true underlying classes, provided they are known. \n",
    "\n",
    "Even when we know the true underlying classes, we do not in genereal know which cluster corresponds to which class, just like we've seen above. The true classes and the estimated clusters would ideally relate to eachother however - one way of relating the two, is to find out which cluster best matches each class, such that each class is assigned one of the clusters, and then calculate the error rate. \n",
    "\n",
    "**Task 3.1:** Is the above definition of the classification error rate for clustering reasonable if we extract the same number of clusters as classes in the data? Does the definition of the classification error make sense when the number of extracted clusters are different from the true underlying classes?\n",
    "\n",
    "- *Answer:*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa36d178",
   "metadata": {},
   "source": [
    "### Validity Measures\n",
    "\n",
    "Rather than using the typical error rate, we will consider the supervised measures of *cluster validity*, in particular the Rand Index, Jaccard Similarity and normalized mutual information (NMI). \n",
    "\n",
    "When we want to compare two clustering partitions, $Q$ and $Z$, we can compare their similarity by looking at pairs of observations. For every pair of points $(i, j)$, we check whether they are placed in the same or in different groups in $Z$ and $Q$.\n",
    "\n",
    "We define:\n",
    "\n",
    "$$\n",
    "S = \\{\\text{Number of pairs in the same cluster in both } Z \\text{ and } Q\\}\n",
    "$$\n",
    "\n",
    "$$\n",
    "D = \\{\\text{Number of pairs in different clusters in both } Z \\text{ and } Q\\}\n",
    "$$\n",
    "\n",
    "The total number of unique pairs is $\\frac{1}{2} N (N-1)$, where $N$ is the number of data points.\n",
    "\n",
    "The **Rand Index** measures the overall agreement between the two clustering partitions, and is thus defined as:\n",
    "\n",
    "$$\n",
    "R(Z, Q) = \\frac{S + D}{\\frac{1}{2}N(N-1)}\n",
    "$$\n",
    "\n",
    "A problem with the Rand index is that if there are many clusters, there will typically be many more pairs of observations in different clusters than in the same cluster and so in general we can expect $D \\gg S$ which means the Rand index is often close to 1. Another measure is therefore the Jaccard similarity, which disregards the pairs in different clusters, $D$.\n",
    "\n",
    "I.e. The **Jaccard similarity** measures agreement, but focuses only on the pairs that are placed *together* in at least one of the partitions (it ignores pairs that are in different clusters in both), and is therefore defined as:\n",
    "\n",
    "$$\n",
    "J(Z, Q) = \\frac{S}{\\frac{1}{2}N(N-1) - D}\n",
    "$$\n",
    "\n",
    "While the Rand and Jaccard indices are based on pair counting, **Normalized Mutual Information** is based on information theory.  \n",
    "It measures how much information one clustering partition provides about the other.\n",
    "\n",
    "To compute this, we first define the joint probability that an observation is assigned to cluster $k$ in partition $Z$ and to cluster $m$ in partition $Q$:\n",
    "\n",
    "$$\n",
    "p_{km}(k, m) = \\frac{n_{km}}{N}, \\quad \\text{for } k = 1, \\ldots, K \\text{ and } m = 1, \\ldots, M\n",
    "$$\n",
    "\n",
    "where $n_{km}$ is the number of observations that belong to cluster $k$ in $Z$ and cluster $m$ in $Q$.\n",
    "\n",
    "From this joint distribution, we can compute the marginal distributions:\n",
    "\n",
    "$$\n",
    "p_k(k) = \\sum_{m=1}^M p_{km}(k, m), \\quad p_m(m) = \\sum_{k=1}^K p_{km}(k, m)\n",
    "$$\n",
    "\n",
    "The entropy of the distributions measures the complexity of the cluster assignments.\n",
    "For a single clustering $Z$, the entropy is:\n",
    "\n",
    "$$\n",
    "H[Z] = -\\sum_{k=1}^K p_k(k) \\log p_k(k)\n",
    "$$\n",
    "\n",
    "Similarly, the joint entropy of the two partitions is:\n",
    "\n",
    "$$\n",
    "H[Z Q] = -\\sum_{k=1}^K \\sum_{m=1}^M p_{km}(k, m) \\log p_{km}(k, m)\n",
    "$$\n",
    "\n",
    "The *mutual information* between $Z$ and $Q$ measures the amount of information shared between the two clusterings - that is, how much knowing one reduces uncertainty about the other:\n",
    "\n",
    "$$\n",
    "\\mathrm{MI}[Z, Q] = H[Z] + H[Q] - H[Z Q]\n",
    "$$\n",
    "\n",
    "Finally, the **Normalized Mutual Information (NMI)** scales this value to lie between 0 and 1, making it easier to interpret and compare across datasets:\n",
    "\n",
    "$$\n",
    "\\mathrm{NMI}[Z, Q] = \\frac{\\mathrm{MI}[Z, Q]}{\\sqrt{H[Z] , H[Q]}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b737ec",
   "metadata": {},
   "source": [
    "**Task 3.2:** Fill in the missing pieces in the function below to calculate the three validity measures.\n",
    "> *Hint:* There are many additional validity measures, check `sklearn.metrics.cluster` - you're welcome to implement more measures:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c3bd9c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "880852c5ad7d29d34205ae73580170c0",
     "grade": false,
     "grade_id": "cell-574b7e4da3758d69",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def clusterval(Q, Z):\n",
    "    # --- Rand index and Jaccard similarity ---\n",
    "    S, D = 0, 0\n",
    "    N = len(Q)\n",
    "    for i in range(N):\n",
    "        for j in range(i):\n",
    "            # Pair i,j belong to same cluster for both Q and Z\n",
    "            if Q[i] == Q[j] and Z[i] == Z[j]:\n",
    "                S += 1\n",
    "            # Pair i,j belong to different clusters for both Q and Z\n",
    "            if Q[i] != Q[j] and Z[i] != Z[j]:\n",
    "                D += 1\n",
    "    \n",
    "    # 3.2) Compute rand index and jaccard similarity from S, D and N\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    # --- NMI ---\n",
    "    # Build contingency table n_km\n",
    "    labels_Z = np.unique(Z)\n",
    "    labels_Q = np.unique(Q)\n",
    "    K = len(labels_Z)\n",
    "    M = len(labels_Q)\n",
    "    n_km = np.zeros((K, M))\n",
    "\n",
    "    # Fill contingency table\n",
    "    for i, k in enumerate(labels_Z):\n",
    "        for j, m in enumerate(labels_Q):\n",
    "            n_km[i, j] = np.sum((Z == k) & (Q == m))\n",
    "\n",
    "    # Convert to joint probability distribution p_km\n",
    "    p_km = n_km / N\n",
    "    # Marginals\n",
    "    p_k = np.sum(p_km, axis=1)\n",
    "    p_m = np.sum(p_km, axis=0)\n",
    "\n",
    "    # Entropy function\n",
    "    def H(p):\n",
    "        EPS = 2.2e-16\n",
    "        return -np.sum(p * np.log(p + EPS))\n",
    "\n",
    "    H_Z = H(p_k)\n",
    "    H_Q = H(p_m)\n",
    "    H_ZQ = H(p_km)\n",
    "\n",
    "    # 3.2) Compute mutual information and normalized MI from H_Z, H_Q, H_ZQ\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return rand, jaccard, NMI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0a5f63",
   "metadata": {},
   "source": [
    "In the following, we evaluate how the clustering quality changes as we vary the number of clusters $K$.\n",
    "For each value of $K = 2, \\ldots, 10$, we perform K-means clustering on the synthetic dataset $\\boldsymbol{X}$, and then compare the resulting cluster partitions to the true class labels $\\boldsymbol{y}$ using the three cluster validity measures introduced above: **Rand Index**, **Jaccard Similarity**, and **Normalized Mutual Information (NMI)**.\n",
    "\n",
    "The results are stored in dictionaries and plotted to visualize how well the clustering aligns with the true labels as the number of clusters increases.\n",
    "\n",
    "**Task 3.3:** Compute the cluster validities in the loop, and store the scores in the provided dictionaries. \n",
    "> *Hint:* If you tried running the previous exercises on the different synthetic datasets, make sure to load `synth1` again for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fddfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load synthetic dataset again\n",
    "dataset_name = \"synth1\"\n",
    "df = pd.read_csv(f\"data/{dataset_name}.csv\")\n",
    "display(df.head())\n",
    "# Separate features and target\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dfa34f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ddb684267909daeaeef01b7032dd1f02",
     "grade": false,
     "grade_id": "cell-648b85a3ec9ef2d8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Maximum number of clusters:\n",
    "K = 10\n",
    "\n",
    "# Allocate variables:\n",
    "Rand, Jaccard, NMI = {}, {}, {}\n",
    "\n",
    "# Run clustering for K=2,...,10\n",
    "for k in range(2, K+1):\n",
    "    # run K-means clustering:\n",
    "    centroids, cluster_ids, _ = k_means(X, k, random_state=3)\n",
    "    # 3.3) compute cluster validities between true labels y and cluster_ids - insert outputs into Rand, Jaccard, NMI dictionaries\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "# Plot results:\n",
    "plt.figure(1)\n",
    "plt.title('Cluster validity')\n",
    "plt.plot(Rand.keys(), Rand.values())\n",
    "plt.plot(Jaccard.keys(), Jaccard.values())\n",
    "plt.plot(NMI.keys(), NMI.values())\n",
    "plt.legend(['Rand', 'Jaccard', 'NMI'], loc=4)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f07f68f",
   "metadata": {},
   "source": [
    "**Task 3.4:** How can the cluster validity measures be used to select the best number of clusters?\n",
    "- *Answer:*\n",
    "\n",
    "**Task 3.5:** What happens when more than four clusters are used to model the data?\n",
    "> *Hint:* Go back to **Task 1.4** and change `K` if you want to see the cluster partitions with different $K$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de39036",
   "metadata": {},
   "source": [
    "For supervised learning we used cross-validation to evaluate performance and estimate the number of parameters in our models, i.e., the number of clusters. Let us assume that we split the data into a training and a test set, train the k-means model on the training set and evaluate how well the model accounts for the test data. Consider evaluating the clustering by summing the distance of test points to the closest estimated cluster center obtained. \n",
    "\n",
    "**Task 3.6:** What do you think will happen with the training and test error as we increase the number of clusters?\n",
    "- *Answer:*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c52e2e",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Part 4: Hierarchical Clustering\n",
    "\n",
    "In this part of the exercise, we will explore agglomerative hierarchical clustering.\n",
    "\n",
    "In agglomerative clustering, we start by treating each data point as its own individual cluster. The algorithm then proceeds bottom-up, repeatedly merging the two clusters that are most similar to each other according to a chosen distance measure.\n",
    "\n",
    "At each step, the distance between clusters can be defined in several ways, through *linkages* — for example:\n",
    "\n",
    "- Minimum (Single) linkage: distance between the closest pair of points in the two clusters.\n",
    "\n",
    "- Maximum (Complete) linkage: distance between the farthest pair of points.\n",
    "\n",
    "- Group average linkage: average distance between all pairs of points across clusters.\n",
    "\n",
    "This merging process continues until all data points belong to a single cluster, forming a hierarchy of nested clusterings. The result is often visualized using a dendrogram, which shows how clusters are combined at different distance thresholds.\n",
    "\n",
    "An example gif is shown below:\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; align-items: center;\">\n",
    "  <img src=\"hierarchical_clustering_illustration.gif\"\n",
    "       alt=\"Hierarchical Clustering Illustration\"\n",
    "       style=\"width: 50%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f3f306",
   "metadata": {},
   "source": [
    "We will once again load the `synth1` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a176f756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load synth1 data and perform hierarchical clustering\n",
    "dataset_name = \"synth1\"\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(f\"data/{dataset_name}.csv\")\n",
    "display(df.head())\n",
    "\n",
    "# Separate features and target\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9806b9d2",
   "metadata": {},
   "source": [
    "**Task 4.1:** Implement the linkage function below, to perform hierarchical clustering with Single linkage and a euclidean distance metric. \n",
    "> *Hint:* Use `Z = linkage(X, method, metric)`.\n",
    "\n",
    "**Task 4.2:** Use `fcluster(Z, criterion=\"maxclust\", t=n_max_clusters)` to assign cluster_ids to the points. Why do we need to set a threshold on the amount of clusters?  \n",
    "- *Answer:*\n",
    "> *Hint:* How many clusters are left at the end of the hierarchical clustering algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb1406e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6e18cb1bdc29fb4f32c02d8fb5343d5d",
     "grade": false,
     "grade_id": "cell-f1a793a3a1c1ecf3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 4.1) Perform hierarchical/agglomerative clustering on data matrix - with Single linkage and Euclidean distance\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# 4.2) Compute cluster ids by thresholding the dendrogram\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "plt.figure(1)\n",
    "clusterplot(X, clusterid=cluster_ids, y=y) \n",
    "# Display dendrogram\n",
    "max_display_levels = 6\n",
    "plt.figure(2, figsize=(10, 4))\n",
    "dendrogram(\n",
    "    Z, truncate_mode=\"level\", p=max_display_levels, color_threshold=Z[-n_max_clusters + 1, 2]\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c597686",
   "metadata": {},
   "source": [
    "**Task 4.3:** Try changing the linkage method and see how it changes the dendrogram. \n",
    "\n",
    "**Task 4.4:** Try running your code several times to see if it generates exactly the same dendrogram each time?\n",
    "> *Hint:* Why might this be different from the k-means algorithm?\n",
    "\n",
    "**Task 4.5:** Try to use the other synthetic datasets, `synth2`, `synth3` and `synth4`. Can you find suitable distance measures for these?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ff1b61",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Part 5: Clustering on the Old Faithful geyser dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957d8847",
   "metadata": {},
   "source": [
    "Old Faithful is a famous geyser located in Yellow Stone national park in the US.\n",
    "\n",
    "The Old Faithful geyser dataset originally described in *Härdle, W. (1991) Smoothing Techniques with Implementation in S. New York*, consists of $N = 272$ observations of two variables, namely the duration of each eruption in minutes (duration) and the waiting time between eruptions also in minutes (waiting). The geysers are categorized as $0 = \\text{small}$ and $1 = \\text{large}$.\n",
    "\n",
    "**Task 5.1:** Load the Old Faithful dataset, found in `data/faithful.csv`, into ($\\boldsymbol{X}$, $\\boldsymbol{y}$).\n",
    "> *Hint:* Load `duration` and `waiting` into `X`.\n",
    "\n",
    "**Task 5.2:** Plot the data using `clusterplot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8b573a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a34f549a9055933929ce861ead79f75b",
     "grade": false,
     "grade_id": "cell-90fcf55a09c59561",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 5.1) Load /data/faithful.csv\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# 5.2) Plot the data using clusterplot\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36747baa",
   "metadata": {},
   "source": [
    "We will now do clustering on the dataset, we will start with K-means.\n",
    "\n",
    "**Task 5.3:** Run K-means on the dataset, and plot the points with their corresponding cluster ids and centroids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d118c4",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b7ebfe012460ab8e962c7175a1f46b49",
     "grade": false,
     "grade_id": "cell-2761654098ce4367",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 5.3) Run K-means on the dataset, and plot the points with their corresponding cluster ids and centroids.\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e32701",
   "metadata": {},
   "source": [
    "**Task 5.4:** Run the `clusterval`-function you created in **Task 3.2** on your cluster ids, to determine the cluster validity measures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdf6d32",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "efaec36476ab81b1d6767d51d64222bb",
     "grade": false,
     "grade_id": "cell-a932d6258f7c9e91",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 5.4) Run the `clusterval`-function you created in **Task 3.2** on your cluster ids, to determine the cluster validity measures.\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98964163",
   "metadata": {},
   "source": [
    "**Task 5.4:** What happens if you increase $K$ beyond the obvious $K=2$? Try to run the program a couple of times (resulting in different initial conditions). Do you see the same solution every time?  \n",
    "\n",
    "The scaling of the variables can seriously affect the results we get in clustering.\n",
    "\n",
    "**Task 5.5:** Discuss whether it is more reasonable to normalize the Old faithful data set? \n",
    "- *Answer:*\n",
    "\n",
    "**Task 5.6:** Try normalizing the data and see whether the results of running `k-means` change.\n",
    "> *Hint:* Use `StandardScaler` from `sklearn.preprocessing` to normalize the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ed996f",
   "metadata": {},
   "source": [
    "We now move on to hierarchical clustering. \n",
    "\n",
    "**Task 5.7:** Run hierarchical clustering on the Old Faithful data using the `single` and `ward` linkage, with and without normalizing the data. Plot the clusterplots and the dendrograms - do you find support for a two cluster model from the structure of the dendrograms?\n",
    "\n",
    "**Task 5.8:** Add the `clusterval`-function to your plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff30b1d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f40b5932911cd39009ce0377446b1cca",
     "grade": false,
     "grade_id": "cell-7e9cf2f889e318a8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 5.7) Perform hierarchical clustering with and without normalization, using both Single and Ward linkages. \n",
    "# Plot dendrograms and cluster plots for each configuration.\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50be167",
   "metadata": {},
   "source": [
    "**Task 5.8:** Explain why it is so important to normalize the data when using Single linkage on this dataset? Relate your answer to your understanding of the different linkage methods. \n",
    "- *Answer:*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
