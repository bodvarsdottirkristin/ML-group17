{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce674305",
   "metadata": {},
   "source": [
    "**02452** *Machine Learning*, Technical University of Denmark\n",
    "\n",
    "- This Jupyter notebook contains exercises where you fill in missing code related to the lecture topic. *First*, try solving each task yourself. *Then* use the provided solution (an HTML file you can open in any web browser) as inspiration if needed. If you get stuck, ask a TA for help.\n",
    "\n",
    "- Some tasks may be difficult or time-consuming - using the solution file or TA support is expected and perfectly fine, as long as you stay active and reflect on the solution.\n",
    "\n",
    "- You are not expected to finish everything during the session. Prepare by looking at the exercises *before* the class, consult the TAs *during* class, and complete the remaining parts *at home*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f63810",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22523f2a",
   "metadata": {},
   "source": [
    "# Week 7: Performance evaluation and AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf35bd00",
   "metadata": {},
   "source": [
    "**Content:** \n",
    "- Part 1: Statistical evaluation of a single classifier (Setup I)\n",
    "- Part 2: Statistical comparison of multiple classifiers (Setup I)\n",
    "- Part 3: Statistical evaluation of a single regression model (Setup I)\n",
    "- Part 4: Statistical comparison of multiple regression models (Setup I)\n",
    "- Part 5: Setup II - Accounting for variability in the training set\n",
    "- Part 6: ROC Curves and AUC\n",
    "\n",
    "**Objectives:**\n",
    "- Understand how to estimate and interpret model performance statistically.\n",
    "- Explain the difference between **Setup I** and **Setup II** for performance evaluation.\n",
    "- Compute and interpret **confidence intervals** for model performance estimates.\n",
    "- Compare models statistically for both **classification** and **regression** tasks.\n",
    "- Understand the motivation and interpretation of the **ROC curve** and **AUC**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60eb5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import LeaveOneOut, KFold, StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn import metrics\n",
    "import scipy.stats as st\n",
    "\n",
    "# Plotting style\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_theme(font_scale=1.)\n",
    "\n",
    "# Random seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "def rocplot(p, y, ax=None, color=\"C0\", label=None):\n",
    "    \"\"\"\n",
    "    Plot the Receiver Operating Characteristic (ROC) curve and compute the AUC.\n",
    "\n",
    "    Args:\n",
    "        p (array-like): Predicted probabilities for the positive class.\n",
    "        y (array-like): True binary class labels (0 or 1).\n",
    "        ax (matplotlib.axes.Axes, optional): Axis to plot on.\n",
    "        color (str, optional): Line color.\n",
    "        label (str, optional): Label for the ROC curve.\n",
    "\n",
    "    Returns:\n",
    "        float: AUC\n",
    "    \"\"\"\n",
    "    fpr, tpr, _ = metrics.roc_curve(y, p)\n",
    "    auc_value = metrics.roc_auc_score(y, p)\n",
    "\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    ax.plot(fpr, tpr, color=color, lw=2, label=f\"AUC = {auc_value:.3f}\")\n",
    "    ax.plot([0, 1], [0, 1], \"k--\", lw=1)\n",
    "    ax.set_xlim([-0.02, 1.02])\n",
    "    ax.set_ylim([-0.02, 1.02])\n",
    "    ax.set_aspect(\"equal\", adjustable=\"box\")\n",
    "    ax.set_xlabel(\"False Positive Rate (1 - Specificity)\")\n",
    "    ax.set_ylabel(\"True Positive Rate (Sensitivity)\")\n",
    "    ax.set_title(\"ROC Curve\")\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "    if label or auc_value:\n",
    "        ax.legend(loc=\"lower right\")\n",
    "\n",
    "    return auc_value\n",
    "\n",
    "def confmatplot(y_true, y_pred, ax=None, cmap=\"Blues\"):\n",
    "    \"\"\"\n",
    "    Plot a styled confusion matrix.\n",
    "\n",
    "    Args:\n",
    "        y_true (array-like): True class labels.\n",
    "        y_pred (array-like): Predicted class labels.\n",
    "        ax (matplotlib.axes.Axes, optional): Axis to plot on.\n",
    "        cmap (str, optional): Colormap for heatmap.\n",
    "    \"\"\"\n",
    "    cm = metrics.confusion_matrix(y_true, y_pred)\n",
    "    acc = 100 * cm.diagonal().sum() / cm.sum()\n",
    "    err = 100 - acc\n",
    "\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    im = ax.imshow(cm, interpolation=\"nearest\", cmap=cmap)\n",
    "    plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "\n",
    "    ax.set_title(f\"Confusion Matrix\\nAccuracy: {acc:.1f}%, Error: {err:.1f}%\")\n",
    "    ax.set_xlabel(\"Predicted label\")\n",
    "    ax.set_ylabel(\"True label\")\n",
    "    ax.set_xticks(np.arange(cm.shape[1]))\n",
    "    ax.set_yticks(np.arange(cm.shape[0]))\n",
    "    ax.set_xticklabels(np.arange(cm.shape[1]))\n",
    "    ax.set_yticklabels(np.arange(cm.shape[0]))\n",
    "\n",
    "    # Add counts in cells\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(\n",
    "                j,\n",
    "                i,\n",
    "                f\"{cm[i, j]}\",\n",
    "                ha=\"center\",\n",
    "                va=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "                fontsize=10,\n",
    "            )\n",
    "\n",
    "    ax.grid(False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38648ed0",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In last week’s lecture and exercises, we explored generalization error, cross-validation, and the problem of overfitting.  \n",
    "This week, we’ll follow it up, by learning how to evaluate and compare model performance statistically. \n",
    "\n",
    "Our goal is to develop objective methods for deciding which model performs best - not just based on a single error estimate, but by also taking into account the uncertainty in those estimates. Relying on a single number can be misleading, especially when models perform similarly.  \n",
    "By quantifying the uncertainty around our performance estimates, we can make more reliable and statistically sound comparisons.\n",
    "\n",
    "In this notebook, you will learn how to:\n",
    "\n",
    "- Estimate the performance of a single model and compute confidence intervals for that estimate.  \n",
    "- Compare multiple models statistically, with confidence intervals, for both classification and regression tasks.  \n",
    "- Understand how different evaluation setups — **Setup I** and **Setup II** — influence how we estimate and interpret performance.  \n",
    "- Evaluate classification models using ROC curves and the Area Under the Curve (AUC).\n",
    "\n",
    "We will begin with **Setup I**, and then move on to **Setup II**, where the correlation between overlapping training data in cross-validation is explicitly taken into account.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a8105a",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Part 1: Statistical evaluation of a single classifier (Setup I)\n",
    "\n",
    "In this part of the exercise, we will explore how to **evaluate and compare classification models** using statistical methods.\n",
    "\n",
    "Specifically, we will address two key questions:\n",
    "\n",
    "1. How can we **estimate a reasonable confidence interval** $\\left[\\theta_L, \\theta_U\\right]$ for the **accuracy of a single classifier**, based on its predictions on a dataset?  \n",
    "2. How can we **compare two classifiers** by estimating **confidence bounds for the difference in their accuracies**, $\\left[\\theta = \\theta_A - \\theta_B\\right]$, where $\\theta_A$ and $\\theta_B$ represent the accuracies of the two classifiers.\n",
    "\n",
    "To keep things simple and interpretable, we will focus on comparing **two k-nearest neighbor (k-NN)** classifiers using the classic **Iris dataset**.  \n",
    "Since this dataset contains only $N = 150$ observations, we will use **leave-one-out** cross-validation (LOO) to obtain $n = N$ model predictions for each classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799f6e76",
   "metadata": {},
   "source": [
    "**Task 1.1:**  Load the Iris dataset into $\\boldsymbol{X}$ and $\\boldsymbol{y}$.\n",
    "> *Hint:* Remember to make $\\boldsymbol{y}$ categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a911e89e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "722de236341b75b86f7d9631e688add0",
     "grade": true,
     "grade_id": "cell-c2d18d467b92903b",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# load iris.csv\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "assert X.shape == (150, 4), \"There should be 150 samples and 4 features in the Iris dataset.\"\n",
    "assert y.shape == (150,), \"There should be 150 labels in the Iris dataset.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ac95b8",
   "metadata": {},
   "source": [
    "First, we will evaluate the **performance of individual models**.\n",
    "We’ll train three k-nearest neighbor (KNN) classifiers with different values of $k: k=1, k=20, k=80$, we'll refer to these as $\\mathcal{M}_A$, $\\mathcal{M}_B$ and $\\mathcal{M}_C$.\n",
    "\n",
    "The script below performs leave-one-out cross-validation (LOO) and saves the predictions from each model in a dictionary called `y_preds`.\n",
    "\n",
    "**Task 1.2:** Initialize Leave-One-Out cross-validation, save it as `CV_loo`.\n",
    "\n",
    "**Task 1.3:** Split the data into training and test sets for the current fold.  \n",
    "> *Hint:* Use `.iloc` with the provided `train_index` and `test_index` to extract the correct rows from `X` and `y`.\n",
    "\n",
    "**Task 1.4:** Fit a KNN model for each value of $k$ within the loop, predict on the test observation, and store the result as `y_pred`.\n",
    "> *Hint:* Use `KNeighborsClassifier`.\n",
    "\n",
    "> *Hint:* We will later use our predictions for **model-comparison**, wherein it is extremely important that the predictions are in the same order for all models. Make sure you always predict with each model within the same CV. Rather than have separate CVs for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e6d9a4",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f9fc72a7e402f1c20fa0aeadf99378ac",
     "grade": true,
     "grade_id": "cell-88bdc2d336cff8be",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Maximum number of neighbors\n",
    "K_neighbours = [1, 20, 80]\n",
    "\n",
    "# 1.2) Initialize Leave-One-Out cross-validation - save it as CV_loo\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# store predictions\n",
    "y_true = []\n",
    "y_preds = {\"Model_A\": [], \"Model_B\": [], \"Model_C\": []}\n",
    "model_names = list(y_preds.keys())\n",
    "\n",
    "# Loop through the folds\n",
    "for fold, (train_index, test_index) in tqdm(enumerate(CV_loo.split(X)), desc=\"Crossvalidation fold\", total=CV_loo.get_n_splits(X)):\n",
    "    \n",
    "    # extract training and test set for current CV fold\n",
    "    # 1.3) Split the data into training and test sets for the current fold - remember iloc\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    # Fit classifier and classify the test points.\n",
    "    for model_idx, k in enumerate(K_neighbours):\n",
    "        # 1.4) Create and fit KNN model with k neighbours. Predict the test set, save as y_pred.\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        # Store predictions in dictionary\n",
    "        y_preds[model_names[model_idx]].append(y_pred)\n",
    "\n",
    "    y_true.append(y_test)\n",
    "\n",
    "# Concatenate the list of arrays into a single array\n",
    "y_true = np.concatenate(y_true)\n",
    "y_preds = {model: np.concatenate(model_preds) for model, model_preds in y_preds.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1a35ad",
   "metadata": {},
   "source": [
    "**Task 1.5:** Compute the accuracy of each of the three models, $\\mathcal{M}_A$, $\\mathcal{M}_B$ and $\\mathcal{M}_C$. Which model performs best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818f7298",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bbba00b06d80bd27480831fdf809e293",
     "grade": true,
     "grade_id": "cell-c89f6adfce978255",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Compute and print accuracies\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fb7ae2",
   "metadata": {},
   "source": [
    "We will now compute **Jeffreys interval**, i.e. the confidence interval of the three models. \n",
    "\n",
    "If $m$ is the number of accurate guesses in $n$ trials, then we can calculate the probability of the classifier being correct: \n",
    "\n",
    "$$\n",
    "p(\\theta \\mid m, n)=\\operatorname{Beta}(\\theta \\mid a, b),\n",
    "$$\n",
    "\n",
    "$$\n",
    " a=m+\\frac{1}{2}, \\text { and } b=n-m+\\frac{1}{2}\n",
    "$$\n",
    "\n",
    "The $1-\\alpha$ confidence interval, $\\left[ \\theta_L, \\theta_U \\right]$, is given as:\n",
    "$$\n",
    "\\theta_L=\\operatorname{cdf}_B^{-1}\\left(\\left.\\frac{\\alpha}{2} \\right\\rvert\\, a, b\\right) \\text { if } m>0 \\text { otherwise } \\theta_L=0,\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta_U=\\operatorname{cdf}_B^{-1}\\left(\\left.1-\\frac{\\alpha}{2} \\right\\rvert\\, a, b\\right) \\text { if } m<n \\text { otherwise } \\theta_U=1\n",
    "$$\n",
    "\n",
    "Such that the probability of $\\theta$ being within the confidence interval is $1-\\alpha$:\n",
    "$$\n",
    "P\\left(\\theta \\in \\left[\\theta_U, \\theta_L \\right] \\right) = 1 - \\alpha\n",
    "$$\n",
    "\n",
    "While the mean is cauculated as follows:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}=\\mathbb{E}[\\theta]=\\frac{a}{a+b}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9100a74",
   "metadata": {},
   "source": [
    "**Task 1.6:** Fill in the `jeffrey_interval` function below, it should return the point estimate `theta_hat` and confidence interval `CI`.\n",
    "> *Hint:* Calculate the number of correct predictions `m`. Use that and `n` to calculate `a` and `b`.\n",
    "\n",
    "> *Hint:* Use the function `st.beta.interval(1 - alpha, a=a, b=b)` to calculate the confidence interval. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7952b1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "99f06ce48900fd19b5680df2a62b7629",
     "grade": true,
     "grade_id": "cell-393a463ea2277834",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def jeffrey_interval(y_true, y_preds, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Calculate the Jeffrey's interval for a binary classification problem.\n",
    "\n",
    "    Parameters:\n",
    "    y_true (numpy.ndarray): The true labels of the binary classification problem.\n",
    "    y_preds (numpy.ndarray): The predicted labels of the binary classification problem.\n",
    "    alpha (float, optional): The significance level for the confidence interval. Default is 0.05.\n",
    "\n",
    "    Returns:\n",
    "    theta_hat (float): The point estimate of the accuracy.\n",
    "    CI (tuple): The lower and upper bounds of the confidence interval.\n",
    "    \"\"\"\n",
    "\n",
    "    # Total number of predictions\n",
    "    n = len(y_true)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return theta_hat, CI\n",
    "\n",
    "# Compute the Jeffreys interval\n",
    "alpha = 0.05\n",
    "\n",
    "for model_name in y_preds.keys():\n",
    "    theta_hat, CI = jeffrey_interval(y_true, y_preds[model_name], alpha=alpha)\n",
    "    print(f\"{model_name}: Theta point estimate {theta_hat:.4f}, CI: [{CI[0]:.4f}, {CI[1]:.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641ec944",
   "metadata": {},
   "source": [
    "**Task 1.7:** Considering the confidence intervals, which is the best performing model now?\n",
    "\n",
    "**Task 1.8:** Try to change $\\alpha$ to $\\alpha = 0.1$ and $\\alpha = 0.01$. What effect does this have on the Jeffrey interval? Explain how $\\alpha$ influences $P\\left(\\alpha \\in \\left[\\theta_L, \\theta_U \\right]\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86946ce7",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Part 2: Statistical comparison of multiple classifiers (Setup I)\n",
    "\n",
    "In the previous part, we evaluated the performance of individual classifiers by estimating their accuracies and their confidence intervals. \n",
    "In this part, we will learn how to **compare two classifiers** in a way that accounts for the uncertainty in their predictions.\n",
    "\n",
    "A common approach for comparing two classification models on the **same dataset** is **McNemar’s test**.  \n",
    "This test is specifically designed for **paired nominal data**, where both models make predictions on the same test samples. It focuses on how often the models disagree on the classification of each observation.\n",
    "\n",
    "The intuition behind McNemar’s test is simple:\n",
    "- We build a **2×2 contingency table** summarizing how often each model was correct or incorrect on the same samples.  \n",
    "- The test then checks whether the number of samples misclassified by one model but correctly classified by the other differs **more than would be expected by chance**.\n",
    "\n",
    "This allows us to formally test the null hypothesis - wherein we want to compare the difference between two classifiers, $\\theta = \\theta_A - \\theta_B$:\n",
    "\n",
    "$$\n",
    "H_0: \\theta = 0\n",
    "$$\n",
    "$$\n",
    "H_1: \\theta \\neq 0\n",
    "$$\n",
    "\n",
    "We start by building the contingency table:\n",
    "\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "n_{11}=\\sum_{i=1}^n c_i^A c_i^B & =\\{\\text { Both classifiers are correct }\\} \\\\\n",
    "n_{12}=\\sum_{k=1}^n c_i^A\\left(1-c_i^B\\right) & =\\{A \\text { is correct, } B \\text { is wrong }\\} \\\\\n",
    "n_{21}=\\sum_{k=1}^n\\left(1-c_i^A\\right) c_i^B & =\\{A \\text { is wrong, } B \\text { is correct }\\} \\\\\n",
    "n_{22}=\\sum_{k=1}^n\\left(1-c_i^A\\right)\\left(1-c_i^B\\right) & =\\{\\text { Both classifiers are wrong }\\}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "wherein $c^A$ and $c^B$ are described as: $c_i= \\begin{cases}1 & \\text { if } \\hat{y}_i=y_i \\\\ 0 & \\text { if otherwise } .\\end{cases}$\n",
    "\n",
    "The distribution of the difference is given as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(\\theta \\mid \\boldsymbol{n}) & =\\frac{1}{2} \\operatorname{Beta}\\left(\\left.\\frac{\\theta+1}{2} \\right\\rvert\\, a=f, b=g\\right), \\\\\n",
    "f & =\\frac{E_\\theta+1}{2}(Q-1) \\quad g=\\frac{1-E_\\theta}{2}(Q-1),\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Wherein the point-estimate $E_\\theta$ and $Q$ are based on how often the models disagree, $n_{12}$ and $n_{21}$, and the total number of predictions, $n$.\n",
    "\n",
    "$$\n",
    "E_\\theta=\\frac{n_{12}-n_{21}}{n}, \\quad Q=\\frac{n^2(n+1)\\left(E_\\theta+1\\right)\\left(1-E_\\theta\\right)}{n\\left(n_{12}+n_{21}\\right)-\\left(n_{12}-n_{21}\\right)^2}\n",
    "$$\n",
    "\n",
    "Finally, we can get the confidence intervals of the difference,\n",
    "\n",
    "$$\n",
    "\\theta_L=2 \\operatorname{cdf}_B^{-1}\\left(\\left.\\frac{\\alpha}{2} \\right\\rvert\\, a=f, b=g\\right)-1, \\quad \\theta_U=2 \\operatorname{cdf}_B^{-1}\\left(\\left.1-\\frac{\\alpha}{2} \\right\\rvert\\, a=f, b=g\\right)-1,\n",
    "$$\n",
    "\n",
    "and the $p$-value, wherein A is better than B if  $n_{12} > n_{21}$:\n",
    "\n",
    "$$\n",
    "p=2 \\operatorname{cdf}_{\\text {binom }}\\left(m=\\min \\left\\{n_{12}, n_{21}\\right\\} \\left\\lvert\\, \\theta=\\frac{1}{2}\\right., N=n_{12}+n_{21}\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "We will now use the predictions from Part 1, to do McNemar’s test and evaluate whether the observed difference in performance between $\\mathcal{M}_A$ and $\\mathcal{M}_B$ is statistically significant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8871b5e",
   "metadata": {},
   "source": [
    "\n",
    "**Task 2.1:** Construct the contingency table `nn`, in the `mcnemar`-function below. \n",
    "The table summarizes how often the two classifiers are correct or incorrect on the same samples.\n",
    "> *Hint:* Start by creating boolean arrays (`cA` and `cB`), indicating whether each classifier’s prediction matches y_true.\n",
    "\n",
    "| |B correct|B wrong|\n",
    "|:-|:-:|:-:|\n",
    "|A correct|$n_{11}$|$n_{12}$|\n",
    "|A wrong|$n_{21}$|$n_{22}$|\n",
    "\n",
    "**Task 2.2:** Compute the estimated difference in accuracy, $E_\\theta$ (`E_theta`), and the intermediate statistic `Q`, from the values in the contingency table.\n",
    "\n",
    "**Task 2.3:** Calculate the parameters `f` and `g` for the corresponding Beta distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce004e06",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ac67a623e307041d5fea8e58dc5a58d1",
     "grade": true,
     "grade_id": "cell-586a4ea54290c1ed",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def mcnemar(y_true, yhatA, yhatB, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Perform McNemar's test to compare the accuracy of two classifiers.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true: array-like, true labels\n",
    "    - yhatA: array-like, predicted labels by classifier A\n",
    "    - yhatB: array-like, predicted labels by classifier B\n",
    "    - alpha: float, significance level (default: 0.05)\n",
    "\n",
    "    Returns:\n",
    "    - E_theta: float, estimated difference in accuracy between classifiers A and B (theta_hat)\n",
    "    - CI: tuple, confidence interval of the estimated difference in accuracy\n",
    "    - p: float, p-value for the two-sided test of whether classifiers A and B have the same accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    # Set up the contingency table\n",
    "    nn = np.zeros((2, 2))\n",
    "\n",
    "    # 2.1) Fill in the contingency table\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    # get values from the contingency table\n",
    "    n = len(y_true)\n",
    "    n12 = nn[0, 1]\n",
    "    n21 = nn[1, 0]\n",
    "\n",
    "    # 2.2) Calculate E_theta and Q from the values in the contingency table\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "    # 2.3) Calculate f and g for the beta distribution\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    # Calculate confidence interval\n",
    "    CI = tuple(bound * 2 - 1 for bound in st.beta.interval(1 - alpha, a=f, b=g))\n",
    "\n",
    "    # Calculate p-value for the two-sided test using exact binomial test\n",
    "    p = 2 * st.binom.cdf(min([n12, n21]), n=n12 + n21, p=0.5)\n",
    "\n",
    "    print(f\"Result of McNemars test using alpha = {alpha}\\n\")\n",
    "    print(\"Contingency table\")\n",
    "    print(nn, \"\\n\")\n",
    "    if n12 + n21 <= 10:\n",
    "        print(\"Warning, n12+n21 is low: n12+n21=\", (n12 + n21))\n",
    "\n",
    "    print(f\"Approximate 1-alpha confidence interval of theta: [thetaL,thetaU] = {CI[0]:.4f}, {CI[1]:.4f}\\n\")\n",
    "    print(\n",
    "        f\"p-value for two-sided test A and B have same accuracy (exact binomial test): p={p}\\n\"\n",
    "    )\n",
    "\n",
    "    return E_theta, CI, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adff1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the Jeffreys interval\n",
    "alpha = 0.05\n",
    "[theta_hat, CI, p] = mcnemar(y_true, y_preds[\"Model_A\"], y_preds[\"Model_B\"], alpha=alpha)\n",
    "\n",
    "print(f\"theta = theta_A-theta_B point estimate: {theta_hat}\\n CI: [{CI[0]:.4f}, {CI[1]:.4f}]\\n p-value: {p}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7dd08a",
   "metadata": {},
   "source": [
    "You should find $\\theta$ being very slightly negative, which is weak evidence towards $\\mathcal{M}_B$ having a relatively higher accuracy than $\\mathcal{M}_A$. Meanwhile, the p-value is relatively high, indicating the result is likely due to chance. All in all the result is inconclusive and we should not conclude $\\mathcal{M}_B$ is better than $\\mathcal{M}_A$.\n",
    "\n",
    "**Task 2.4:** Run the function above again, to compare $\\mathcal{M}_A$ against $\\mathcal{M}_C$. Based on these results, are there statistically good reasons to believe $\\mathcal{M}_A$ is better than $\\mathcal{M}_C$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af65f80",
   "metadata": {},
   "source": [
    "**Task 2.5:** Go back to the cross-validation code-block, replace $\\mathcal{M}_B$ with a classification tree model. Use McNemar’s test to compare the decision tree against a KNN classifier with k = 1 ($\\mathcal{M}_A$). Comment on the result of the evaluation. Is one model better than the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c044bd59",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Part 3: Statistical evaluation of a single regression model (Setup I)\n",
    "\n",
    "We will now move on to **evaluate and compare regression models** using statistical methods.\n",
    "\n",
    "Specifically, we will address two key questions:\n",
    "\n",
    "1. How can we **estimate a reasonable confidence interval** $\\left[z_L, z_U\\right]$ for the **performance of a single regression model**, based on its predictions on a dataset?  \n",
    "2. How can we **compare two regression models** by estimating confidence bounds for the difference in their performances.\n",
    "\n",
    "Since we do not have many candidate regression models to choose from at this point, we will compare the **linear regression** model against a **regression tree**, where we will predict alcohol content in the **Wine dataset**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad07d48a",
   "metadata": {},
   "source": [
    "**Task 3.1:**  Load the Wine dataset into $\\boldsymbol{X}$ and $\\boldsymbol{y}$ (`Alcohol`), don't use `Color` and `Quality score (0-10)`.\n",
    "> *Hint:* Remember to filter outliers: `df = df[~((df['Volatile acidity'] > 2) | (df['Density'] > 1) | (df['Alcohol'] > 20))]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e93c7b3",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "481ec2a6d19ed5ba1568644c17cb275d",
     "grade": true,
     "grade_id": "cell-1999925dafb46331",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "assert X.shape == (6304, 10), \"There should be 6304 samples and 10 features in the Wine dataset.\"\n",
    "assert y.shape == (6304,), \"There should be 6304 labels in the Wine dataset.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf55279a",
   "metadata": {},
   "source": [
    "First, we will evaluate the **performance of individual models**.\n",
    "We’ll train a linear regression model and a decision tree regressor, we'll refer to these as $\\mathcal{M}_A$ and $\\mathcal{M}_B$.\n",
    "\n",
    "The script below performs 10-fold cross-validation and saves the predictions from each model in a dictionary called `y_preds`.\n",
    "\n",
    "**Task 3.2:** Initialize 10-fold cross-validation, save it as `CV_kfold`.\n",
    "\n",
    "**Task 3.3:** Split the data into training and test sets for the current fold.  \n",
    "> *Hint:* Use `.iloc` with the provided `train_index` and `test_index` to extract the correct rows from `X` and `y`.\n",
    "\n",
    "**Task 3.4:** Fit a linear regression model and a decision tree regressor, predict on the test set, and store the result as `y_pred_A` and `y_pred_B`.\n",
    "> *Hint:* Use `LinearRegression` and `DecisionTreeRegressor` with default parameters.\n",
    "\n",
    "> *Hint:* We will later use our predictions for **model-comparison**, wherein it is extremely important that the predictions are in the same order for all models. Make sure you always predict with each model within the same CV. Rather than have separate CVs for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb587cbf",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1cb0a96e319d52b72242f331e853789d",
     "grade": true,
     "grade_id": "cell-046e19f36484831d",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 3.2) Initialize 10-fold cross-validation - save it as CV_kfold\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# store predictions\n",
    "y_true = []\n",
    "y_preds = {\"Model_A\": [], \"Model_B\": []}\n",
    "model_names = list(y_preds.keys())\n",
    "\n",
    "# Loop through the folds\n",
    "for fold, (train_index, test_index) in tqdm(enumerate(CV_kfold.split(X)), desc=\"Crossvalidation fold\", total=CV_kfold.get_n_splits(X)):\n",
    "    \n",
    "    # extract training and test set for current CV fold\n",
    "    # 3.3) Split the data into training and test sets for the current fold - remember iloc\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    # 3.4) Create and fit a Linear Regression model (model_A) and a DecisionTreeRegressor (model_B) with default parameters. \n",
    "    # Predict the test set, save as y_pred_A and y_pred_B.\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    y_preds[\"Model_A\"].append(y_preds_A)\n",
    "    y_preds[\"Model_B\"].append(y_preds_B)\n",
    "\n",
    "    # Append the true labels\n",
    "    y_true.append(y_test)\n",
    "\n",
    "# Concatenate the list of arrays into a single array\n",
    "y_true = np.concatenate(y_true)\n",
    "y_preds = {model: np.concatenate(model_preds) for model, model_preds in y_preds.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24ea0b2",
   "metadata": {},
   "source": [
    "We will once again compute the confidence intervals of our models. We'll start by computing our estimated error: \n",
    "$$\n",
    "\\hat{z}=\\frac{1}{n} \\sum_{i=1}^n z_i\n",
    "$$ \n",
    "\n",
    "Picking L1 or L2-loss, we have $z_i$ given as:\n",
    "\n",
    "$$\n",
    "z_i=\\left|\\hat{y}_i-y_i\\right| \\quad \\text { or } \\quad z_i=\\left(\\hat{y}_i-y_i\\right)^2\n",
    "$$\n",
    "\n",
    "Assuming our losses are normally-distributed, $z_i \\sim \\mathcal{N}\\left(z_i \\mid \\mu=u, \\sigma^2\\right)$, our lower- and upper-bound are given as a student's t-distribution:\n",
    "\n",
    "$$\n",
    "z_L = \\operatorname{cdf}_{\\mathcal{T}}^{-1}\\!\\left(\\left.\\frac{\\alpha}{2} \\,\\right\\rvert\\, \\nu, \\hat{z}, \\tilde{\\sigma}\\right) \\\\\n",
    "z_U = \\operatorname{cdf}_{\\mathcal{T}}^{-1}\\!\\left(\\left.1 - \\frac{\\alpha}{2} \\,\\right\\rvert\\, \\nu, \\hat{z}, \\tilde{\\sigma}\\right)\n",
    "$$\n",
    "\n",
    "with degrees of freedom $\\nu=n-1$ and standard error of the mean $\\tilde{\\sigma}=\\sqrt{\\sum_{i=1}^n \\frac{\\left(z_i-\\hat{z}\\right)^2}{n(n-1)}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b44748b",
   "metadata": {},
   "source": [
    "We will now compute confidence intervals for our model errors based on the chosen loss function (L1 or L2). \n",
    "The goal is to quantify the uncertainty in our estimated performance, $\\hat{z}$, using the **Student’s t-distribution**.\n",
    "\n",
    "The function `confidence_interval()` below should compute:\n",
    "1. The estimated error $\\hat{z}$\n",
    "2. The confidence interval $[Z_L, Z_U]$, assuming normally distributed losses.\n",
    "\n",
    "**Task 3.5:** Compute the errors, $z$, and the estimated average error, $\\hat{z}=\\frac{1}{n} \\sum_{i=1}^n z_i$, in the function below.\n",
    "> *Hint:* Apply the chosen loss function (`loss_fn`) to `y_true` and `y_preds` for `z` then take the mean using `np.mean()` to get `z_hat`.\n",
    "\n",
    "**Task 3.6:** Compute the **standard error of the mean**, using the formula: $\\tilde{\\sigma}=\\sqrt{\\sum_{i=1}^n \\frac{\\left(z_i-\\hat{z}\\right)^2}{n(n-1)}}$\n",
    "> *Hint:* You can calculate this directly with NumPy operations.\n",
    "\n",
    "\n",
    "**Task 3.7:** Compute the **confidence interval** using the Student’s t-distribution.  \n",
    "> *Hint:* Use `st.t.interval(confidence, df, loc, scale)`, where:`confidence = 1 - alpha`, `df = n - 1`, `loc = z_hat` and `scale = sem`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be259f6",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9e836ac880bee0c67263f1f4c887f8eb",
     "grade": true,
     "grade_id": "cell-572649d2034320da",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Defining our loss functions\n",
    "l1_loss = lambda y, y_pred: np.abs(y - y_pred)\n",
    "l2_loss = lambda y, y_pred: (y - y_pred)**2\n",
    "\n",
    "\n",
    "def confidence_interval(y_true, y_preds, loss_fn, alpha=0.05):\n",
    "\n",
    "    # 3.5) Calculate estimated error, z_hat, as the mean loss across all samples\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # n and nu\n",
    "    n = len(y_true)\n",
    "    nu = n - 1  # degrees of freedom\n",
    "\n",
    "    # 3.6) Calculate standard error of the mean of the losses (sem)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # 3.7) Calculate the confidence interval\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return z_hat, CI\n",
    "\n",
    "# Compute the confidence interval\n",
    "alpha = 0.05\n",
    "\n",
    "for model_name in y_preds.keys():\n",
    "    z_hat, CI = confidence_interval(y_true, y_preds[model_name], l2_loss, alpha=alpha)\n",
    "    print(f\"{model_name}: z_hat: {z_hat:.4f}, CI: [{CI[0]:.4f}, {CI[1]:.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e5fe9a",
   "metadata": {},
   "source": [
    "**Task 3.8** Considering the confidence intervals, which is the best performing model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb709329",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Part 4: Statistical comparison of multiple regression models (Setup I)\n",
    "\n",
    "In the previous part, we evaluated the performance of individual regression models by estimating their performance and their confidence intervals. \n",
    "In this part, we will learn how to **compare two classifiers** in a way that accounts for the uncertainty in their predictions.\n",
    "\n",
    "We now let $\\hat{z}$ be the difference in loss between the two models, $\\hat{z}=\\frac{1}{n} \\sum_{i=1}^n z_i$, where $z_i=z_i^A-z_i^B$.\n",
    "\n",
    "We once again assume $z_i$ to be normally distributed,  $z_i \\sim \\mathcal{N}\\left(z_i \\mid \\mu=u, \\sigma^2\\right)$ - which essentially means that $\\hat{Z}$ and the confidence intervals can be computed through the exact same formulas as before. \n",
    "\n",
    "Finally, our hypotheses become:\n",
    "\n",
    "$H_0:$ Model $\\mathcal{M}_A$ and $\\mathcal{M}_B$ have the same performance, $u=0$\n",
    "\n",
    "$H_1:$ Model $\\mathcal{M}_A$ and $\\mathcal{M}_B$ have different performance, $u \\neq 0$\n",
    "\n",
    "Wherein the $p$-value is given from the students t-distribution: \n",
    "$$p=2 \\operatorname{cdf}_{\\mathcal{T}}(-|\\hat{z}| \\mid \\nu=n-1, \\mu=0, \\sigma=\\tilde{\\sigma})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5fd916",
   "metadata": {},
   "source": [
    "**Task 4.1:** Compute the difference in errors, $z$, and the estimated average difference in errors error, $\\hat{z}=\\frac{1}{n} \\sum_{i=1}^n z_i$, where $z_i=z_i^A-z_i^B$, in the function below.\n",
    "> *Hint:* Apply the chosen loss function (`loss_fn`) to `y_true` and `y_preds` for each model, and subtract them to get `z`, then take the mean using `np.mean()` to get `z_hat`.\n",
    "\n",
    "**Task 4.2:** Insert the solution from **Task 3.6** to calculate $\\tilde{\\sigma}=\\sqrt{\\sum_{i=1}^n \\frac{\\left(z_i-\\hat{z}\\right)^2}{n(n-1)}}$.\n",
    "\n",
    "**Task 4.3:** Insert the solution from **Task 3.7** to calculate the confidence interval. \n",
    "\n",
    "**Task 4.4:** Calculate the p-value.\n",
    "> *Hint:* You can use `2 * st.t.cdf(t_stat, df=nu)`, wherein `t_stat` is given as: $t = \\frac{\\hat{z} - \\mu_0}{\\tilde{\\sigma}}$, in the null-hypothesis our hypothesized mean, $\\mu_0 = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e871f6",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dccef1128006d769318a225b92a65400",
     "grade": true,
     "grade_id": "cell-574ddb12ecda1be2",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def confidence_interval_comparison(y_true, y_preds_A, y_preds_B, loss_fn, alpha=0.05):\n",
    "\n",
    "    # 4.1) Calculate estimated error, z_hat, as the mean loss across all samples\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # n and nu\n",
    "    n = len(y_true)\n",
    "    nu = n - 1  # degrees of freedom\n",
    "\n",
    "    # 4.2) Insert solution from task 3.6 here\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # 4.3) Insert solution from task 3.7 here\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    # 4.4) Calculate the p-value for the two-sided test using the t-distribution\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return z_hat, CI, p_value\n",
    "\n",
    "z_hat, CI, p_value = confidence_interval_comparison(y_true, y_preds[\"Model_A\"], y_preds[\"Model_B\"], l2_loss, alpha=alpha)\n",
    "print(f\"Difference in loss between Model_A and Model_B: \\nz_hat: {z_hat:.4f}, \\nCI: [{CI[0]:.4f}, {CI[1]:.4f}], \\np-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766b801a",
   "metadata": {},
   "source": [
    "**Task 4.4:** What is the confidence interval and p-value, and what would you conclude based on these results?\n",
    "\n",
    "**Task 4.5:** Try leave-one-out cross-validation instead, what happens?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37785ea",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Part 5: Setup II - Accounting for variability in the training set\n",
    "\n",
    "So far, in **Setup I**, we have only considered performance evaluation under the assumption that our model is **trained on a fixed training set**.  \n",
    "In that setup, all statistical estimates — such as generalization error, confidence intervals, and model comparisons — describe the behavior of a model trained on that *specific dataset*.  \n",
    "\n",
    "However, even if we could perfectly evaluate that model’s performance (for example, with an infinite test set), we would still miss an important source of variability:  \n",
    "> What happens if we train the same model on a *different* training set drawn from the same population?\n",
    "\n",
    "In **Setup II**, we no longer treat the training data as fixed.  \n",
    "Instead, we consider the generalization error averaged over *all possible training sets of size N*:\n",
    "\n",
    "$$\n",
    "E_{\\text{gen}} = \n",
    "\\int\\!\\!\\int L(f_{D_{\\text{train}}}(x), y)\\,dx\\,dy\\,dD_{\\text{train}}\n",
    "$$\n",
    "\n",
    "This means we are now interested in the **expected performance of the learning algorithm itself**, not just a single trained model.\n",
    "\n",
    "In practice, we approximate this by training our model on several (ideally independent) training/test splits:\n",
    "$$\n",
    "(D_{\\text{train}}^{(1)}, D_{\\text{test}}^{(1)}), \\ldots, (D_{\\text{train}}^{(J)}, D_{\\text{test}}^{(J)}),\n",
    "$$\n",
    "computing one generalization error estimate for each of these $J$ splits:\n",
    "$$\n",
    "r_j = \\frac{1}{n_j} \\sum_{i=1}^{n_j} L(f_{D_{\\text{train}}^{(j)}}(x_i^{(j)}), y_i^{(j)}),\n",
    "$$\n",
    "and then taking their average:\n",
    "$$\n",
    "\\hat{r} = \\frac{1}{J} \\sum_{j=1}^J r_j.\n",
    "$$\n",
    "\n",
    "In practice, we often obtain these $J$ splits using **cross-validation**, exactly like we've been doing in all of the previous exercises today. But this is what creates the problem, \n",
    "as the training sets $D_{\\text{train}}^{(j)}$ in cross-validation **overlap heavily**, so the resulting models (and their error estimates $r_j$) are not independent.\n",
    "\n",
    "As the training data across datasets appear in multiple cross-validation splits, it is no surprise that they are correlated by some $\\Sigma$:\n",
    "\n",
    "$$\n",
    "r_j = \\bar{z} + v_j, \\quad v \\sim \\mathcal{N}(0, \\Sigma),\n",
    "$$\n",
    "\n",
    "\n",
    "The covariance matrix can be described through its diagonal and off-diagonal elements - where, $\\sigma^2$ represents the **true variance** in test errors due to noise in data, and $\\rho$ represents the (unknown) **correlation** between estimates caused by overlapping training sets.\n",
    "\n",
    "$$\n",
    "\\Sigma_{ii} = \\sigma^2, \\quad \\Sigma_{ij} = \\rho \\sigma^2.\n",
    "$$\n",
    "\n",
    "Since it is not possible to estimate both $\\sigma^2$ and $\\rho$ directly from one dataset (Bengio & Grandvalet, 2004), we use a **correlation heuristic** proposed by Nadeau & Bengio (2000):\n",
    "\n",
    "$$\n",
    "\\rho = \\frac{|D_{\\text{test}}|}{|D_{\\text{train}}| + |D_{\\text{test}}|} = \\frac{1}{K}\n",
    "$$\n",
    "\n",
    "for $K$-fold cross-validation.  \n",
    "\n",
    "This heuristic accounts for the dependency between training sets while keeping the analysis tractable.  \n",
    "Under this model, the variance of our estimated mean performance becomes:\n",
    "\n",
    "$$\n",
    "\\tilde{\\sigma}^2 =\n",
    "\\left( \\frac{1}{J} + \\frac{1}{K - 1} \\right)\n",
    "\\hat{s}^2,\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\hat{s}^2 = \\frac{1}{J - 1} \\sum_{j=1}^{J} (r_j - \\bar{r})^2\n",
    "$$\n",
    "is the observed variance across the $J$ repeated cross-validation estimates.\n",
    "\n",
    "**In practice**, though it might sound complicated, this is in fact quite simple to estimate. As the training sets within our cross-validation folds are all overlapping, we choose to repeat the cross-validation with different seeds, such that we don't run just $K$ folds - but instead run our cross-validation $m$ independent times, for a total of $J = mK$ folds. \n",
    "\n",
    "We then compute our statistical measures through a correlated t-test.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc4d772",
   "metadata": {},
   "source": [
    "**Task 5.1:** Look at the correlated ttest function below, and make sure you understand it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89ed5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlated_ttest(r, rho, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Perform a correlated t-test to compare two models under Setup II.\n",
    "\n",
    "    Parameters:\n",
    "    - r (array-like): Array of performance differences across folds (e.g. r_j = error_A - error_B)\n",
    "    - rho (float): Correlation coefficient between folds (typically 1/K for K-fold CV)\n",
    "    - alpha (float, optional): Significance level (default: 0.05)\n",
    "\n",
    "    Returns:\n",
    "    - p (float): p-value of the test\n",
    "    - CI (tuple): Confidence interval for the mean difference\n",
    "    \"\"\"\n",
    "\n",
    "    r = np.array(r)\n",
    "    r_hat = np.mean(r)\n",
    "    s_hat = np.std(r, ddof=1)\n",
    "    J = len(r)\n",
    "\n",
    "    # Adjusted standard deviation accounting for correlation\n",
    "    sigma_tilde = s_hat * np.sqrt((1 / J) + (rho / (1 - rho)))\n",
    "\n",
    "    # Confidence interval\n",
    "    CI = st.t.interval(1 - alpha, df=J - 1, loc=r_hat, scale=sigma_tilde)\n",
    "\n",
    "    # Two-sided p-value\n",
    "    p = 2 * st.t.cdf(-np.abs(r_hat) / sigma_tilde, df=J - 1)\n",
    "\n",
    "    return r_hat, CI, p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c32b986",
   "metadata": {},
   "source": [
    "**Task 5.2:** Initialize 10-fold cross-validation within the repetition loop.\n",
    "> *Hint:* Make sure you set a different seed for each repetition!\n",
    "\n",
    "**Task 5.3:** Compute the mean difference in loss between models, $r_j$, for each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79181897",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2b5b87a0e731022757a9b83e22a65076",
     "grade": true,
     "grade_id": "cell-0025260d77e840f7",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "m = 3 # Repetitions\n",
    "K = 10 # Folds\n",
    "rho = 1 / K # Correlation heuristic\n",
    "loss_func = l2_loss # Loss function\n",
    "\n",
    "r = []\n",
    "\n",
    "for repeat_idx in range(m):\n",
    "    print(f\"Repetition {repeat_idx+1}/{m}\")\n",
    "\n",
    "    # 5.2) Initialize KFold cross-validation, set the seed to repeat_idx\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    for fold, (train_index, test_index) in tqdm(enumerate(CV_kfold.split(X)), total=CV_kfold.get_n_splits(X),desc=\"Cross-validation fold\"):\n",
    "        # Split data into training and test sets\n",
    "        X_train = X.iloc[train_index, :]\n",
    "        y_train = y.iloc[train_index]\n",
    "        X_test = X.iloc[test_index, :]\n",
    "        y_test = y.iloc[test_index]\n",
    "\n",
    "        # Fit models: Linear Regression and Decision Tree Regressor\n",
    "        model_A = LinearRegression().fit(X_train, y_train)\n",
    "        model_B = DecisionTreeRegressor(random_state=repeat_idx).fit(X_train, y_train)\n",
    "\n",
    "        # Predict test data\n",
    "        ypred_A = model_A.predict(X_test)\n",
    "        ypred_B = model_B.predict(X_test)\n",
    "\n",
    "        # 5.3) Compute mean difference in loss between models for this fold, r_j\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        r.append(r_j)\n",
    "\n",
    "# Calculate p-value and confidence interval using correlated t-test\n",
    "r_hat, CI, p_value = correlated_ttest(r, rho, alpha=alpha)\n",
    "\n",
    "print(f\"\\nSetup II results:\")\n",
    "print(f\"r_hat: {r_hat:.4f}\")\n",
    "print(f\"95% CI: [{CI[0]:.4f}, {CI[1]:.4f}]\")\n",
    "print(f\"p-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfcec2e",
   "metadata": {},
   "source": [
    "**Task 5.4:** What is the confidence interval and p-value, and what would you conclude based on these results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f681347",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Part 6: ROC Curves and AUC\n",
    "\n",
    "So far, we have evaluated classification performance using metrics such as accuracy, loss, and confidence intervals - all based on discrete class predictions.  \n",
    "However, many classifiers (such as **logistic regression** or **neural networks**) output a **continuous score** or **probability**, which we later convert into class labels using a threshold (e.g., 0.5).  \n",
    "\n",
    "By varying this threshold, we can observe how the tradeoff between correctly identifying positive examples and incorrectly labeling negatives changes.  \n",
    "This relationship is summarized in the **Receiver Operating Characteristic (ROC)** curve.\n",
    "\n",
    "The **ROC curve** is a graphical tool for evaluating the performance of **binary classifiers**.  \n",
    "\n",
    "It plots:\n",
    "\n",
    "- The **True Positive Rate (TPR)** = Sensitivity = Recall  \n",
    "$$\n",
    "\\text{TPR} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n",
    "$$\n",
    "\n",
    "against\n",
    "\n",
    "- The **False Positive Rate (FPR)** = 1 − Specificity  \n",
    "$$\n",
    "\\text{FPR} = \\frac{\\text{FP}}{\\text{FP} + \\text{TN}}\n",
    "$$\n",
    "\n",
    "based on TP, FP, TN, FN:\n",
    "- **True Positives (TP):** Positive samples correctly predicted as positive  \n",
    "- **False Positives (FP):** Negative samples incorrectly predicted as positive  \n",
    "- **True Negatives (TN):** Negative samples correctly predicted as negative  \n",
    "- **False Negatives (FN):** Positive samples incorrectly predicted as negative \n",
    "\n",
    "for all possible classification thresholds.\n",
    "\n",
    "Each point on the ROC curve corresponds to a different threshold value, showing the tradeoff between catching positives and avoiding false positives.\n",
    "\n",
    "To summarize the ROC curve in a single number, we often use the **Area Under the Curve (AUC)**.  \n",
    "- A model with **AUC = 0.5** performs no better than random guessing.  \n",
    "- A model with **AUC = 1.0** achieves perfect classification.  \n",
    "\n",
    "A higher AUC indicates that the model is better at ranking positive samples higher than negative ones, regardless of the threshold.\n",
    "\n",
    "In this exercise, we will:\n",
    "1. Load the **Wine2 dataset** (red vs. white wines) - Wine2 is just Wine1 with removed outliers.  \n",
    "2. Train a **logistic regression** classifier using **stratified 50/50 train-test splits** to ensure balanced class proportions.  \n",
    "3. Plot the ROC curve and compute the corresponding AUC score.\n",
    "\n",
    "We will treat the **red wines as the positive class** and the **white wines as the negative class**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04242a3d",
   "metadata": {},
   "source": [
    "**Task 6.1:** Load the Wine dataset into $\\boldsymbol{X}$ and $\\boldsymbol{y}$ (`Color`).\n",
    "> *Hint:* Make `y` categorical, then access binary labels through `.codes`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92347c1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "edc88662d5e4e68fe054aa12038f862f",
     "grade": true,
     "grade_id": "cell-1d92a2640e324037",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 6.1) load data/wine2.csv, split into X and y (Color)\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11369b73",
   "metadata": {},
   "source": [
    "**Task 6.2:** Initialize stratified K-fold cross-validation with $K=2$, save it as `CV_kfold`.\n",
    "> *Hint:* Use `StratifiedKFold`, remember to shuffle. \n",
    "\n",
    "**Task 6.3:** Create and fit a LogisticRegression model with default parameters. Predict on the test set, save as `y_pred` and predicted probabilities as `p`.\n",
    "> *Hint:* You have previously used logistic regression on this dataset in Week 5.  \n",
    "> *Hint:* You can get the probabilities through `model.predict_proba(X_test)[:,1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d421c359",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "37fa6321cbc9d250e1c37911a1a2ab1c",
     "grade": true,
     "grade_id": "cell-9851abc7ece2da9b",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 6.2) Initialize StratifiedKFold cross-validation with 2 folds - save it as CV_kfold\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(CV_kfold.split(X,y)):\n",
    "    # extract training and test set for current CV fold\n",
    "    X_train, y_train = X.iloc[train_index,:], y[train_index]\n",
    "    X_test, y_test = X.iloc[test_index,:], y[test_index]\n",
    "\n",
    "    # 6.3) Create and fit a LogisticRegression model with default parameters. \n",
    "    # Predict on the test set, save as y_pred and predicted probabilities as p.\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    # Create figure with 2 subplots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "    # ROC curve\n",
    "    rocplot(p, y_test, ax=axes[0])\n",
    "\n",
    "    # Confusion matrix\n",
    "    confmatplot(y_test, y_pred, ax=axes[1])\n",
    "\n",
    "    fig.suptitle(f\"Fold {fold+1}\", fontsize=13, fontweight=\"bold\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07a9c00",
   "metadata": {},
   "source": [
    "**Task 6.4:** The ROC curve lies very close to the top-left corner. What does this shape tell you about the tradeoff between true positives and false positives for your classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632e68f5",
   "metadata": {},
   "source": [
    "In the previous task, you used all available features to classify wines as red or white and observed an almost perfect ROC curve with an AUC close to 1.  \n",
    "In this task, we repeat the experiment - but this time, we will train the classifier using only a single attribute: “Alcohol.”\n",
    "\n",
    "This will allow us to examine how much information this single feature carries about the wine type, and how this is reflected in the **ROC curve** and the **AUC** score.\n",
    "\n",
    "**Task 6.5:**  \n",
    "Restrict your training and test data to include only the **“Alcohol”** attribute - and run the above code again. \n",
    "> *Hint:* You can select the column directly using `X[\"Alcohol\"]` or similar indexing - you might have to use `.to_frame()` afterwards, to make the array 2-dimensional.\n",
    "\n",
    "**Task 6.6:** How can you tell from the ROC curve and the AUC that alcohol content alone is not very useful for distinguishing between red and white wines? Does a single feature provide enough information for reliable classification?\n",
    "\n",
    "**Task 6.7:** When using logistic regression, do you think it is always best to include **as many attributes as possible**?  \n",
    "> *Hint:* Consider the potential trade-offs, such as overfitting, correlated features, and model interpretability.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
